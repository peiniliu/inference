{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"MLPerf\u2122 Inference Benchmark Suite","text":"<p>MLPerf Inference is a benchmark suite for measuring how fast systems can run models in a variety of deployment scenarios. </p> <p>Please see the MLPerf Inference benchmark paper for a detailed description of the benchmarks along with the motivation and guiding principles behind the benchmark suite. If you use any part of this benchmark (e.g., reference implementations, submissions, etc.), please cite the following:</p> <pre><code>@misc{reddi2019mlperf,\n    title={MLPerf Inference Benchmark},\n    author={Vijay Janapa Reddi and Christine Cheng and David Kanter and Peter Mattson and Guenther Schmuelling and Carole-Jean Wu and Brian Anderson and Maximilien Breughe and Mark Charlebois and William Chou and Ramesh Chukka and Cody Coleman and Sam Davis and Pan Deng and Greg Diamos and Jared Duke and Dave Fick and J. Scott Gardner and Itay Hubara and Sachin Idgunji and Thomas B. Jablin and Jeff Jiao and Tom St. John and Pankaj Kanwar and David Lee and Jeffery Liao and Anton Lokhmotov and Francisco Massa and Peng Meng and Paulius Micikevicius and Colin Osborne and Gennady Pekhimenko and Arun Tejusve Raghunath Rajan and Dilip Sequeira and Ashish Sirasao and Fei Sun and Hanlin Tang and Michael Thomson and Frank Wei and Ephrem Wu and Lingjie Xu and Koichi Yamada and Bing Yu and George Yuan and Aaron Zhong and Peizhao Zhang and Yuchen Zhou},\n    year={2019},\n    eprint={1911.02549},\n    archivePrefix={arXiv},\n    primaryClass={cs.LG}\n}\n</code></pre>"},{"location":"#mlperf-inference-v40-submission-deadline-february-23-2024","title":"MLPerf Inference v4.0 (submission deadline February 23, 2024)","text":"<p>There is an extra one-week extension allowed only for the llama2-70b submissions. For submissions, please use the master branch and any commit since the 4.0 seed release although it is best to use the latest commit. v4.0 tag will be created from the master branch after the result publication.</p> <p>For power submissions please use SPEC PTD 1.10 (needs special access) and any commit of the power-dev repository after the code-freeze</p> model reference app framework dataset category resnet50-v1.5 vision/classification_and_detection tensorflow, onnx, tvm, ncnn imagenet2012 edge,datacenter retinanet 800x800 vision/classification_and_detection pytorch, onnx openimages resized to 800x800 edge,datacenter bert language/bert tensorflow, pytorch, onnx squad-1.1 edge,datacenter dlrm-v2 recommendation/dlrm_v2 pytorch Multihot Criteo Terabyte datacenter 3d-unet vision/medical_imaging/3d-unet-kits19 pytorch, tensorflow, onnx KiTS19 edge,datacenter rnnt speech_recognition/rnnt pytorch OpenSLR LibriSpeech Corpus edge,datacenter gpt-j language/gpt-j pytorch CNN-Daily Mail edge,datacenter stable-diffusion-xl text_to_image pytorch COCO 2014 edge,datacenter llama2-70b language/llama2-70b pytorch OpenOrca datacenter <ul> <li>Framework here is given for the reference implementation. Submitters are free to use their own frameworks to run the benchmark.</li> </ul>"},{"location":"#mlperf-inference-v31-submission-august-18-2023","title":"MLPerf Inference v3.1 (submission August 18, 2023)","text":"<p>Please use v3.1 tag (<code>git checkout v3.1</code>) if you would like to reproduce the v3.1 results. </p> <p>For reproducing power submissions please use the <code>master</code> branch of the MLCommons power-dev repository and checkout to e9e16b1299ef61a2a5d8b9abf5d759309293c440. </p> <p>You can see the individual README files in the benchmark task folders for more details regarding the benchmarks. For reproducing the submitted results please see the README files under the respective submitter folders in the inference v3.1 results repository.</p> model reference app framework dataset category resnet50-v1.5 vision/classification_and_detection tensorflow, onnx, tvm, ncnn imagenet2012 edge,datacenter retinanet 800x800 vision/classification_and_detection pytorch, onnx openimages resized to 800x800 edge,datacenter bert language/bert tensorflow, pytorch, onnx squad-1.1 edge,datacenter dlrm-v2 recommendation/dlrm_v2 pytorch Multihot Criteo Terabyte datacenter 3d-unet vision/medical_imaging/3d-unet-kits19 pytorch, tensorflow, onnx KiTS19 edge,datacenter rnnt speech_recognition/rnnt pytorch OpenSLR LibriSpeech Corpus edge,datacenter gpt-j language/gpt-j pytorch CNN-Daily Mail edge,datacenter"},{"location":"#mlperf-inference-v30-submission-03032023","title":"MLPerf Inference v3.0 (submission 03/03/2023)","text":"<p>Please use the v3.0 tag (<code>git checkout v3.0</code>) if you would like to reproduce v3.0 results.</p> <p>You can see the individual Readme files in the reference app for more details.</p> model reference app framework dataset category resnet50-v1.5 vision/classification_and_detection tensorflow, onnx, tvm imagenet2012 edge,datacenter retinanet 800x800 vision/classification_and_detection pytorch, onnx openimages resized to 800x800 edge,datacenter bert language/bert tensorflow, pytorch, onnx squad-1.1 edge,datacenter dlrm recommendation/dlrm pytorch, tensorflow Criteo Terabyte datacenter 3d-unet vision/medical_imaging/3d-unet-kits19 pytorch, tensorflow, onnx KiTS19 edge,datacenter rnnt speech_recognition/rnnt pytorch OpenSLR LibriSpeech Corpus edge,datacenter"},{"location":"#mlperf-inference-v21-submission-08052022","title":"MLPerf Inference v2.1 (submission 08/05/2022)","text":"<p>Use the r2.1 branch (<code>git checkout r2.1</code>) if you want to submit or reproduce v2.1 results.</p> <p>See the individual Readme files in the reference app for details.</p> model reference app framework dataset category resnet50-v1.5 vision/classification_and_detection tensorflow, onnx imagenet2012 edge,datacenter retinanet 800x800 vision/classification_and_detection pytorch, onnx openimages resized to 800x800 edge,datacenter bert language/bert tensorflow, pytorch, onnx squad-1.1 edge,datacenter dlrm recommendation/dlrm pytorch, tensorflow Criteo Terabyte datacenter 3d-unet vision/medical_imaging/3d-unet-kits19 pytorch, tensorflow, onnx KiTS19 edge,datacenter rnnt speech_recognition/rnnt pytorch OpenSLR LibriSpeech Corpus edge,datacenter"},{"location":"#mlperf-inference-v20-submission-02252022","title":"MLPerf Inference v2.0 (submission 02/25/2022)","text":"<p>Use the r2.0 branch (<code>git checkout r2.0</code>) if you want to submit or reproduce v2.0 results.</p> <p>See the individual Readme files in the reference app for details.</p> model reference app framework dataset category resnet50-v1.5 vision/classification_and_detection tensorflow, onnx imagenet2012 edge,datacenter ssd-mobilenet 300x300 vision/classification_and_detection tensorflow, pytorch, onnx coco resized to 300x300 edge ssd-resnet34 1200x1200 vision/classification_and_detection tensorflow, pytorch, onnx coco resized to 1200x1200 edge,datacenter bert language/bert tensorflow, pytorch, onnx squad-1.1 edge,datacenter dlrm recommendation/dlrm pytorch, tensorflow Criteo Terabyte datacenter 3d-unet vision/medical_imaging/3d-unet-kits19 pytorch, tensorflow, onnx KiTS19 edge,datacenter rnnt speech_recognition/rnnt pytorch OpenSLR LibriSpeech Corpus edge,datacenter"},{"location":"#mlperf-inference-v11-submission-08132021","title":"MLPerf Inference v1.1 (submission 08/13/2021)","text":"<p>Use the r1.1 branch (<code>git checkout r1.1</code>) if you want to submit or reproduce v1.1 results.</p> <p>See the individual Readme files in the reference app for details.</p> model reference app framework dataset category resnet50-v1.5 vision/classification_and_detection tensorflow, onnx imagenet2012 edge,datacenter ssd-mobilenet 300x300 vision/classification_and_detection tensorflow, pytorch, onnx coco resized to 300x300 edge ssd-resnet34 1200x1200 vision/classification_and_detection tensorflow, pytorch, onnx coco resized to 1200x1200 edge,datacenter bert language/bert tensorflow, pytorch, onnx squad-1.1 edge,datacenter dlrm recommendation/dlrm pytorch, tensorflow Criteo Terabyte datacenter 3d-unet vision/medical_imaging/3d-unet pytorch, tensorflow(?), onnx(?) BraTS 2019 edge,datacenter rnnt speech_recognition/rnnt pytorch OpenSLR LibriSpeech Corpus edge,datacenter"},{"location":"#mlperf-inference-v10-submission-03192021","title":"MLPerf Inference v1.0 (submission 03/19/2021)","text":"<p>Use the r1.0 branch (<code>git checkout r1.0</code>) if you want to submit or reproduce v1.0 results.</p> <p>See the individual Readme files in the reference app for details.</p> model reference app framework dataset category resnet50-v1.5 vision/classification_and_detection tensorflow, onnx imagenet2012 edge,datacenter ssd-mobilenet 300x300 vision/classification_and_detection tensorflow, pytorch, onnx coco resized to 300x300 edge ssd-resnet34 1200x1200 vision/classification_and_detection tensorflow, pytorch, onnx coco resized to 1200x1200 edge,datacenter bert language/bert tensorflow, pytorch, onnx squad-1.1 edge,datacenter dlrm recommendation/dlrm pytorch, tensorflow(?) Criteo Terabyte datacenter 3d-unet vision/medical_imaging/3d-unet pytorch, tensorflow(?), onnx(?) BraTS 2019 edge,datacenter rnnt speech_recognition/rnnt pytorch OpenSLR LibriSpeech Corpus edge,datacenter"},{"location":"#mlperf-inference-v07-submission-9182020","title":"MLPerf Inference v0.7 (submission 9/18/2020)","text":"<p>Use the r0.7 branch (<code>git checkout r0.7</code>) if you want to submit or reproduce v0.7 results.</p> <p>See the individual Readme files in the reference app for details.</p> model reference app framework dataset resnet50-v1.5 vision/classification_and_detection tensorflow, pytorch, onnx imagenet2012 ssd-mobilenet 300x300 vision/classification_and_detection tensorflow, pytorch, onnx coco resized to 300x300 ssd-resnet34 1200x1200 vision/classification_and_detection tensorflow, pytorch, onnx coco resized to 1200x1200 bert language/bert tensorflow, pytorch, onnx squad-1.1 dlrm recommendation/dlrm pytorch, tensorflow(?), onnx(?) Criteo Terabyte 3d-unet vision/medical_imaging/3d-unet pytorch, tensorflow(?), onnx(?) BraTS 2019 rnnt speech_recognition/rnnt pytorch OpenSLR LibriSpeech Corpus"},{"location":"#mlperf-inference-v05","title":"MLPerf Inference v0.5","text":"<p>Use the r0.5 branch (<code>git checkout r0.5</code>) if you want to reproduce v0.5 results.</p> <p>See the individual Readme files in the reference app for details.</p> model reference app framework dataset resnet50-v1.5 v0.5/classification_and_detection tensorflow, pytorch, onnx imagenet2012 mobilenet-v1 v0.5/classification_and_detection tensorflow, pytorch, onnx imagenet2012 ssd-mobilenet 300x300 v0.5/classification_and_detection tensorflow, pytorch, onnx coco resized to 300x300 ssd-resnet34 1200x1200 v0.5/classification_and_detection tensorflow, pytorch, onnx coco resized to 1200x1200 gnmt v0.5/translation/gnmt/ tensorflow, pytorch See Readme"},{"location":"benchmarks/","title":"MLPerf Inference Benchmarks","text":"<p>Please visit the individual benchmark links to see the run commands using the unified CM interface.</p> <ol> <li> <p>Image Classification using ResNet50 model and Imagenet-2012 dataset</p> </li> <li> <p>Text to Image using Stable Diffusion model and Coco2014 dataset</p> </li> <li> <p>Object Detection using Retinanet model and OpenImages dataset</p> </li> <li> <p>Image Segmentation  using 3d-unet model and KiTS19 dataset</p> </li> <li> <p>Question Answering using Bert-Large model and Squad v1.1 dataset</p> </li> <li> <p>Text Summarization using GPT-J model and CNN Daily Mail dataset</p> </li> <li> <p>Text Summarization using LLAMA2-70b model and OpenORCA dataset</p> </li> <li> <p>Recommendation using DLRMv2 model and Criteo multihot dataset</p> </li> </ol> <p>All the eight benchmarks can participate in the datacenter category. All the eight benchmarks except DLRMv2 and LLAMA2 and can participate in the edge category. </p> <p><code>bert</code>, <code>llama2-70b</code>, <code>dlrm_v2</code> and <code>3d-unet</code> has a high accuracy (99.9%) variant, where the benchmark run  must achieve a higher accuracy of at least <code>99.9%</code> of the FP32 reference model in comparison with the <code>99%</code> default accuracy requirement.</p> <p>The <code>dlrm_v2</code> benchmark has a high-accuracy variant only. If this accuracy is not met, the submission result can be submitted only to the open division.</p>"},{"location":"benchmarks/image_classification/resnet50/","title":"Image Classification using ResNet50","text":""},{"location":"benchmarks/image_classification/resnet50/#dataset","title":"Dataset","text":"<p>The benchmark implementation run command will automatically download the validation and calibration datasets and do the necessary preprocessing. In case you want to download only the datasets, you can use the below commands.</p> ValidationCalibration <p>ResNet50 validation run uses the Imagenet 2012 validation dataset consisting of 50,000 images.</p> <p>ResNet50 calibration dataset consist of 500 images selected from the Imagenet 2012 validation dataset. There are 2 alternative options for the calibration dataset.</p>"},{"location":"benchmarks/image_classification/resnet50/#get-validation-dataset","title":"Get Validation Dataset","text":"<pre><code>cm run script --tags=get,dataset,imagenet,validation -j\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#get-calibration-dataset-using-option-1","title":"Get Calibration Dataset Using Option 1","text":"<pre><code>cm run script --tags=get,dataset,imagenet,calibration,_mlperf.option1 -j\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#get-calibration-dataset-using-option-2","title":"Get Calibration Dataset Using Option 2","text":"<pre><code>cm run script --tags=get,dataset,imagenet,calibration,_mlperf.option2 -j\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#model","title":"Model","text":"<p>The benchmark implementation run command will automatically download the required model and do the necessary conversions. In case you want to only download the official model, you can use the below commands.</p> <p>Get the Official MLPerf ResNet50 Model</p> TensorflowOnnx"},{"location":"benchmarks/image_classification/resnet50/#tensorflow","title":"Tensorflow","text":"<pre><code>cm run script --tags=get,ml-model,resnet50,_tensorflow -j\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#onnx","title":"Onnx","text":"<pre><code>cm run script --tags=get,ml-model,resnet50,_onnx -j\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#benchmark-implementations","title":"Benchmark Implementations","text":"MLCommons-PythonNvidiaIntelQualcommMLCommon-C++"},{"location":"benchmarks/image_classification/resnet50/#mlperf-reference-implementation-in-python","title":"MLPerf Reference Implementation in Python","text":"edgedatacenter"},{"location":"benchmarks/image_classification/resnet50/#edge-category","title":"Edge category","text":"<p>In the edge category, resnet50 has Offline, SingleStream, Multistream scenarios and all the scenarios are mandatory for a closed division submission.</p> OnnxruntimeTensorflowDeepsparse"},{"location":"benchmarks/image_classification/resnet50/#onnxruntime-framework","title":"Onnxruntime framework","text":"CPUCUDAROCm"},{"location":"benchmarks/image_classification/resnet50/#cpu-device","title":"CPU device","text":""},{"location":"benchmarks/image_classification/resnet50/#docker-setup-command","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamMultistreamAll Scenarios"},{"location":"benchmarks/image_classification/resnet50/#offline","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#singlestream","title":"# SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_1","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#multistream","title":"# Multistream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Multistream \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_2","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge  \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_3","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#cuda-device","title":"CUDA device","text":""},{"location":"benchmarks/image_classification/resnet50/#docker-setup-command_1","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineSingleStreamMultistreamAll Scenarios"},{"location":"benchmarks/image_classification/resnet50/#offline_1","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_4","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#singlestream_1","title":"# SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_5","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#multistream_1","title":"# Multistream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Multistream \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_6","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_1","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge  \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_7","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#rocm-device","title":"ROCm device","text":""},{"location":"benchmarks/image_classification/resnet50/#docker-setup-command_2","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=rocm  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamMultistreamAll Scenarios"},{"location":"benchmarks/image_classification/resnet50/#offline_2","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_8","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#singlestream_2","title":"# SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_9","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#multistream_2","title":"# Multistream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Multistream \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_10","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_2","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge  \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_11","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#tensorflow-framework","title":"Tensorflow framework","text":"CPUCUDAROCm"},{"location":"benchmarks/image_classification/resnet50/#cpu-device_1","title":"CPU device","text":""},{"location":"benchmarks/image_classification/resnet50/#docker-setup-command_3","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamMultistreamAll Scenarios"},{"location":"benchmarks/image_classification/resnet50/#offline_3","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_12","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#singlestream_3","title":"# SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_13","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#multistream_3","title":"# Multistream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=Multistream \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_14","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_3","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge  \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_15","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#cuda-device_1","title":"CUDA device","text":""},{"location":"benchmarks/image_classification/resnet50/#docker-setup-command_4","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineSingleStreamMultistreamAll Scenarios"},{"location":"benchmarks/image_classification/resnet50/#offline_4","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_16","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#singlestream_4","title":"# SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_17","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#multistream_4","title":"# Multistream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=Multistream \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_18","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_4","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge  \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_19","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#rocm-device_1","title":"ROCm device","text":""},{"location":"benchmarks/image_classification/resnet50/#docker-setup-command_5","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=rocm  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamMultistreamAll Scenarios"},{"location":"benchmarks/image_classification/resnet50/#offline_5","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_20","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#singlestream_5","title":"# SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_21","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#multistream_5","title":"# Multistream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=Multistream \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_22","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_5","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge  \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_23","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#deepsparse-framework","title":"Deepsparse framework","text":"CPU"},{"location":"benchmarks/image_classification/resnet50/#cpu-device_2","title":"CPU device","text":""},{"location":"benchmarks/image_classification/resnet50/#docker-setup-command_6","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamMultistreamAll Scenarios"},{"location":"benchmarks/image_classification/resnet50/#offline_6","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_24","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#singlestream_6","title":"# SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_25","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#multistream_6","title":"# Multistream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge \\\n   --scenario=Multistream \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_26","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_6","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=edge  \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_27","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#datacenter-category","title":"Datacenter category","text":"<p>In the datacenter category, resnet50 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> OnnxruntimeTensorflowDeepsparse"},{"location":"benchmarks/image_classification/resnet50/#onnxruntime-framework_1","title":"Onnxruntime framework","text":"CPUCUDAROCm"},{"location":"benchmarks/image_classification/resnet50/#cpu-device_3","title":"CPU device","text":""},{"location":"benchmarks/image_classification/resnet50/#docker-setup-command_7","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/image_classification/resnet50/#offline_7","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_28","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#server","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_29","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_7","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_30","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#cuda-device_2","title":"CUDA device","text":""},{"location":"benchmarks/image_classification/resnet50/#docker-setup-command_8","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/image_classification/resnet50/#offline_8","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_31","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#server_1","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_32","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_8","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_33","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#rocm-device_2","title":"ROCm device","text":""},{"location":"benchmarks/image_classification/resnet50/#docker-setup-command_9","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=rocm  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/image_classification/resnet50/#offline_9","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_34","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#server_2","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_35","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_9","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_36","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#tensorflow-framework_1","title":"Tensorflow framework","text":"CPUCUDAROCm"},{"location":"benchmarks/image_classification/resnet50/#cpu-device_4","title":"CPU device","text":""},{"location":"benchmarks/image_classification/resnet50/#docker-setup-command_10","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/image_classification/resnet50/#offline_10","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_37","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#server_3","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_38","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_10","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_39","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#cuda-device_3","title":"CUDA device","text":""},{"location":"benchmarks/image_classification/resnet50/#docker-setup-command_11","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/image_classification/resnet50/#offline_11","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_40","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#server_4","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_41","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_11","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_42","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#rocm-device_3","title":"ROCm device","text":""},{"location":"benchmarks/image_classification/resnet50/#docker-setup-command_12","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=rocm  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/image_classification/resnet50/#offline_12","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_43","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#server_5","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_44","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_12","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_45","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#deepsparse-framework_1","title":"Deepsparse framework","text":"CPU"},{"location":"benchmarks/image_classification/resnet50/#cpu-device_5","title":"CPU device","text":""},{"location":"benchmarks/image_classification/resnet50/#docker-setup-command_13","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/image_classification/resnet50/#offline_13","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_46","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#server_6","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_47","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_13","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=reference \\\n   --framework=deepsparse \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_48","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#nvidia-mlperf-implementation","title":"Nvidia MLPerf Implementation","text":"edgedatacenter"},{"location":"benchmarks/image_classification/resnet50/#edge-category_1","title":"Edge category","text":"<p>In the edge category, resnet50 has Offline, SingleStream, Multistream scenarios and all the scenarios are mandatory for a closed division submission.</p> TensorRT"},{"location":"benchmarks/image_classification/resnet50/#tensorrt-framework","title":"TensorRT framework","text":"CUDA"},{"location":"benchmarks/image_classification/resnet50/#cuda-device_4","title":"CUDA device","text":""},{"location":"benchmarks/image_classification/resnet50/#docker-setup-command_14","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=resnet50 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineSingleStreamMultistreamAll Scenarios"},{"location":"benchmarks/image_classification/resnet50/#offline_14","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=resnet50 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_49","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#singlestream_7","title":"# SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=resnet50 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_50","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#multistream_7","title":"# Multistream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=resnet50 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Multistream \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_51","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_14","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge  \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_52","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#datacenter-category_1","title":"Datacenter category","text":"<p>In the datacenter category, resnet50 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> TensorRT"},{"location":"benchmarks/image_classification/resnet50/#tensorrt-framework_1","title":"TensorRT framework","text":"CUDA"},{"location":"benchmarks/image_classification/resnet50/#cuda-device_5","title":"CUDA device","text":""},{"location":"benchmarks/image_classification/resnet50/#docker-setup-command_15","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=resnet50 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/image_classification/resnet50/#offline_15","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=resnet50 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_53","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#server_7","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=resnet50 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_54","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_15","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_55","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#intel-mlperf-implementation","title":"Intel MLPerf Implementation","text":"edgedatacenter"},{"location":"benchmarks/image_classification/resnet50/#edge-category_2","title":"Edge category","text":"<p>In the edge category, resnet50 has Offline, SingleStream, Multistream scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/image_classification/resnet50/#pytorch-framework","title":"Pytorch framework","text":"CPU"},{"location":"benchmarks/image_classification/resnet50/#cpu-device_6","title":"CPU device","text":""},{"location":"benchmarks/image_classification/resnet50/#docker-setup-command_16","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=resnet50 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamMultistreamAll Scenarios"},{"location":"benchmarks/image_classification/resnet50/#offline_16","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=resnet50 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_56","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#singlestream_8","title":"# SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=resnet50 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_57","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#multistream_8","title":"# Multistream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=resnet50 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Multistream \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_58","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_16","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_59","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#datacenter-category_2","title":"Datacenter category","text":"<p>In the datacenter category, resnet50 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/image_classification/resnet50/#pytorch-framework_1","title":"Pytorch framework","text":"CPU"},{"location":"benchmarks/image_classification/resnet50/#cpu-device_7","title":"CPU device","text":""},{"location":"benchmarks/image_classification/resnet50/#docker-setup-command_17","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=resnet50 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/image_classification/resnet50/#offline_17","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=resnet50 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_60","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#server_8","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=resnet50 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_61","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_17","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_62","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#qualcomm-ai100-mlperf-implementation","title":"Qualcomm AI100 MLPerf Implementation","text":"edgedatacenter"},{"location":"benchmarks/image_classification/resnet50/#edge-category_3","title":"Edge category","text":"<p>In the edge category, resnet50 has Offline, SingleStream, Multistream scenarios and all the scenarios are mandatory for a closed division submission.</p> Glow"},{"location":"benchmarks/image_classification/resnet50/#glow-framework","title":"Glow framework","text":"QAIC"},{"location":"benchmarks/image_classification/resnet50/#qaic-device","title":"QAIC device","text":""},{"location":"benchmarks/image_classification/resnet50/#docker-setup-command_18","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=resnet50 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=qaic  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamMultistreamAll Scenarios"},{"location":"benchmarks/image_classification/resnet50/#offline_18","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=resnet50 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=qaic \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_63","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#singlestream_9","title":"# SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=resnet50 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution-mode=valid \\\n   --device=qaic \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_64","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#multistream_9","title":"# Multistream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=resnet50 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=edge \\\n   --scenario=Multistream \\\n   --execution-mode=valid \\\n   --device=qaic \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_65","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_18","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=edge  \\\n   --execution-mode=valid \\\n   --device=qaic \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_66","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#datacenter-category_3","title":"Datacenter category","text":"<p>In the datacenter category, resnet50 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Glow"},{"location":"benchmarks/image_classification/resnet50/#glow-framework_1","title":"Glow framework","text":"QAIC"},{"location":"benchmarks/image_classification/resnet50/#qaic-device_1","title":"QAIC device","text":""},{"location":"benchmarks/image_classification/resnet50/#docker-setup-command_19","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=resnet50 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=qaic  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/image_classification/resnet50/#offline_19","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=resnet50 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=qaic \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_67","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#server_9","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=resnet50 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=qaic \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_68","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_19","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=qaic \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_69","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#mlperf-modular-implementation-in-c","title":"MLPerf Modular Implementation in C++","text":"edgedatacenter"},{"location":"benchmarks/image_classification/resnet50/#edge-category_4","title":"Edge category","text":"<p>In the edge category, resnet50 has Offline, SingleStream, Multistream scenarios and all the scenarios are mandatory for a closed division submission.</p> Onnxruntime"},{"location":"benchmarks/image_classification/resnet50/#onnxruntime-framework_2","title":"Onnxruntime framework","text":"CPUCUDA"},{"location":"benchmarks/image_classification/resnet50/#cpu-device_8","title":"CPU device","text":""},{"location":"benchmarks/image_classification/resnet50/#docker-setup-command_20","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamMultistreamAll Scenarios"},{"location":"benchmarks/image_classification/resnet50/#offline_20","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_70","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#singlestream_10","title":"# SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_71","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#multistream_10","title":"# Multistream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Multistream \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_72","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_20","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge  \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_73","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#cuda-device_6","title":"CUDA device","text":""},{"location":"benchmarks/image_classification/resnet50/#docker-setup-command_21","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineSingleStreamMultistreamAll Scenarios"},{"location":"benchmarks/image_classification/resnet50/#offline_21","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_74","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#singlestream_11","title":"# SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_75","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#multistream_11","title":"# Multistream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Multistream \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_76","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_21","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge  \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_77","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#datacenter-category_4","title":"Datacenter category","text":"<p>In the datacenter category, resnet50 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Onnxruntime"},{"location":"benchmarks/image_classification/resnet50/#onnxruntime-framework_3","title":"Onnxruntime framework","text":"CPUCUDA"},{"location":"benchmarks/image_classification/resnet50/#cpu-device_9","title":"CPU device","text":""},{"location":"benchmarks/image_classification/resnet50/#docker-setup-command_22","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/image_classification/resnet50/#offline_22","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_78","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#server_10","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_79","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_22","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_80","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#cuda-device_7","title":"CUDA device","text":""},{"location":"benchmarks/image_classification/resnet50/#docker-setup-command_23","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/image_classification/resnet50/#offline_23","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_81","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#server_11","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_82","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/image_classification/resnet50/#all-scenarios_23","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=resnet50 \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/image_classification/resnet50/#run-options_83","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/","title":"Question Answering using Bert-Large","text":""},{"location":"benchmarks/language/bert/#dataset","title":"Dataset","text":"<p>The benchmark implementation run command will automatically download the validation and calibration datasets and do the necessary preprocessing. In case you want to download only the datasets, you can use the below commands.</p> Validation <p>BERT validation run uses the SQuAD v1.1 dataset.</p>"},{"location":"benchmarks/language/bert/#get-validation-dataset","title":"Get Validation Dataset","text":"<pre><code>cm run script --tags=get,dataset,squad,validation -j\n</code></pre>"},{"location":"benchmarks/language/bert/#model","title":"Model","text":"<p>The benchmark implementation run command will automatically download the required model and do the necessary conversions. In case you want to only download the official model, you can use the below commands.</p> <p>Get the Official MLPerf Bert-Large Model</p> PytorchOnnxTensorflow"},{"location":"benchmarks/language/bert/#pytorch","title":"Pytorch","text":"<pre><code>cm run script --tags=get,ml-model,bert-large,_pytorch -j\n</code></pre>"},{"location":"benchmarks/language/bert/#onnx","title":"Onnx","text":"<pre><code>cm run script --tags=get,ml-model,bert-large,_onnx -j\n</code></pre>"},{"location":"benchmarks/language/bert/#tensorflow","title":"Tensorflow","text":"<pre><code>cm run script --tags=get,ml-model,bert-large,_tensorflow -j\n</code></pre>"},{"location":"benchmarks/language/bert/#benchmark-implementations","title":"Benchmark Implementations","text":"MLCommons-PythonNvidiaIntelQualcomm"},{"location":"benchmarks/language/bert/#mlperf-reference-implementation-in-python","title":"MLPerf Reference Implementation in Python","text":"<p>BERT-99</p> edgedatacenter"},{"location":"benchmarks/language/bert/#edge-category","title":"Edge category","text":"<p>In the edge category, bert-99 has Offline, SingleStream scenarios and all the scenarios are mandatory for a closed division submission.</p> OnnxruntimePytorchTensorflow"},{"location":"benchmarks/language/bert/#onnxruntime-framework","title":"Onnxruntime framework","text":"CPUCUDAROCm"},{"location":"benchmarks/language/bert/#cpu-device","title":"CPU device","text":""},{"location":"benchmarks/language/bert/#docker-setup-command","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamAll Scenarios"},{"location":"benchmarks/language/bert/#offline","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#singlestream","title":"# SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_1","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#all-scenarios","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge  \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_2","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#cuda-device","title":"CUDA device","text":""},{"location":"benchmarks/language/bert/#docker-setup-command_1","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineSingleStreamAll Scenarios"},{"location":"benchmarks/language/bert/#offline_1","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_3","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#singlestream_1","title":"# SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_4","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#all-scenarios_1","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge  \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_5","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#rocm-device","title":"ROCm device","text":""},{"location":"benchmarks/language/bert/#docker-setup-command_2","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=rocm  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamAll Scenarios"},{"location":"benchmarks/language/bert/#offline_2","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_6","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#singlestream_2","title":"# SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_7","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#all-scenarios_2","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge  \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_8","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#pytorch-framework","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"benchmarks/language/bert/#cpu-device_1","title":"CPU device","text":""},{"location":"benchmarks/language/bert/#docker-setup-command_3","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamAll Scenarios"},{"location":"benchmarks/language/bert/#offline_3","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_9","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#singlestream_3","title":"# SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_10","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#all-scenarios_3","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_11","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#cuda-device_1","title":"CUDA device","text":""},{"location":"benchmarks/language/bert/#docker-setup-command_4","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineSingleStreamAll Scenarios"},{"location":"benchmarks/language/bert/#offline_4","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_12","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#singlestream_4","title":"# SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_13","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#all-scenarios_4","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_14","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#rocm-device_1","title":"ROCm device","text":""},{"location":"benchmarks/language/bert/#docker-setup-command_5","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=rocm  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamAll Scenarios"},{"location":"benchmarks/language/bert/#offline_5","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_15","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#singlestream_5","title":"# SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_16","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#all-scenarios_5","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_17","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#tensorflow-framework","title":"Tensorflow framework","text":"CPUCUDAROCm"},{"location":"benchmarks/language/bert/#cpu-device_2","title":"CPU device","text":""},{"location":"benchmarks/language/bert/#docker-setup-command_6","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamAll Scenarios"},{"location":"benchmarks/language/bert/#offline_6","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_18","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#singlestream_6","title":"# SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_19","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#all-scenarios_6","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge  \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_20","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#cuda-device_2","title":"CUDA device","text":""},{"location":"benchmarks/language/bert/#docker-setup-command_7","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineSingleStreamAll Scenarios"},{"location":"benchmarks/language/bert/#offline_7","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_21","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#singlestream_7","title":"# SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_22","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#all-scenarios_7","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge  \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_23","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#rocm-device_2","title":"ROCm device","text":""},{"location":"benchmarks/language/bert/#docker-setup-command_8","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=rocm  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamAll Scenarios"},{"location":"benchmarks/language/bert/#offline_8","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_24","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#singlestream_8","title":"# SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_25","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#all-scenarios_8","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=edge  \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_26","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#datacenter-category","title":"Datacenter category","text":"<p>In the datacenter category, bert-99 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> OnnxruntimePytorchTensorflow"},{"location":"benchmarks/language/bert/#onnxruntime-framework_1","title":"Onnxruntime framework","text":"CPUCUDAROCm"},{"location":"benchmarks/language/bert/#cpu-device_3","title":"CPU device","text":""},{"location":"benchmarks/language/bert/#docker-setup-command_9","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/language/bert/#offline_9","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_27","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#server","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_28","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#all-scenarios_9","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_29","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#cuda-device_3","title":"CUDA device","text":""},{"location":"benchmarks/language/bert/#docker-setup-command_10","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/language/bert/#offline_10","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_30","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#server_1","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_31","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#all-scenarios_10","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_32","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#rocm-device_3","title":"ROCm device","text":""},{"location":"benchmarks/language/bert/#docker-setup-command_11","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=rocm  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/language/bert/#offline_11","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_33","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#server_2","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_34","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#all-scenarios_11","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_35","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#pytorch-framework_1","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"benchmarks/language/bert/#cpu-device_4","title":"CPU device","text":""},{"location":"benchmarks/language/bert/#docker-setup-command_12","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/language/bert/#offline_12","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_36","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#server_3","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_37","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#all-scenarios_12","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_38","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#cuda-device_4","title":"CUDA device","text":""},{"location":"benchmarks/language/bert/#docker-setup-command_13","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/language/bert/#offline_13","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_39","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#server_4","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_40","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#all-scenarios_13","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_41","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#rocm-device_4","title":"ROCm device","text":""},{"location":"benchmarks/language/bert/#docker-setup-command_14","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=rocm  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/language/bert/#offline_14","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_42","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#server_5","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_43","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#all-scenarios_14","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_44","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#tensorflow-framework_1","title":"Tensorflow framework","text":"CPUCUDAROCm"},{"location":"benchmarks/language/bert/#cpu-device_5","title":"CPU device","text":""},{"location":"benchmarks/language/bert/#docker-setup-command_15","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/language/bert/#offline_15","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_45","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#server_6","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_46","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#all-scenarios_15","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_47","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#cuda-device_5","title":"CUDA device","text":""},{"location":"benchmarks/language/bert/#docker-setup-command_16","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/language/bert/#offline_16","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_48","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#server_7","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_49","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#all-scenarios_16","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_50","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#rocm-device_5","title":"ROCm device","text":""},{"location":"benchmarks/language/bert/#docker-setup-command_17","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=rocm  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios <p>BERT-99.9</p> datacenter"},{"location":"benchmarks/language/bert/#offline_17","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_51","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#server_8","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_52","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#all-scenarios_17","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_53","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#datacenter-category_1","title":"Datacenter category","text":"<p>In the datacenter category, bert-99.9 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> OnnxruntimePytorchTensorflow"},{"location":"benchmarks/language/bert/#onnxruntime-framework_2","title":"Onnxruntime framework","text":"CPUCUDAROCm"},{"location":"benchmarks/language/bert/#cpu-device_6","title":"CPU device","text":""},{"location":"benchmarks/language/bert/#docker-setup-command_18","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/language/bert/#offline_18","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_54","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#server_9","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_55","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#all-scenarios_18","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_56","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#cuda-device_6","title":"CUDA device","text":""},{"location":"benchmarks/language/bert/#docker-setup-command_19","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/language/bert/#offline_19","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_57","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#server_10","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_58","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#all-scenarios_19","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_59","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#rocm-device_6","title":"ROCm device","text":""},{"location":"benchmarks/language/bert/#docker-setup-command_20","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=rocm  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/language/bert/#offline_20","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_60","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#server_11","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_61","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#all-scenarios_20","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_62","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#pytorch-framework_2","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"benchmarks/language/bert/#cpu-device_7","title":"CPU device","text":""},{"location":"benchmarks/language/bert/#docker-setup-command_21","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/language/bert/#offline_21","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_63","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#server_12","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_64","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#all-scenarios_21","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_65","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#cuda-device_7","title":"CUDA device","text":""},{"location":"benchmarks/language/bert/#docker-setup-command_22","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/language/bert/#offline_22","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_66","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#server_13","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_67","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#all-scenarios_22","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_68","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#rocm-device_7","title":"ROCm device","text":""},{"location":"benchmarks/language/bert/#docker-setup-command_23","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=rocm  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/language/bert/#offline_23","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_69","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#server_14","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_70","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#all-scenarios_23","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_71","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#tensorflow-framework_2","title":"Tensorflow framework","text":"CPUCUDAROCm"},{"location":"benchmarks/language/bert/#cpu-device_8","title":"CPU device","text":""},{"location":"benchmarks/language/bert/#docker-setup-command_24","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/language/bert/#offline_24","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_72","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#server_15","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_73","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#all-scenarios_24","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_74","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#cuda-device_8","title":"CUDA device","text":""},{"location":"benchmarks/language/bert/#docker-setup-command_25","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/language/bert/#offline_25","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_75","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#server_16","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_76","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#all-scenarios_25","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_77","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#rocm-device_8","title":"ROCm device","text":""},{"location":"benchmarks/language/bert/#docker-setup-command_26","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=rocm  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/language/bert/#offline_26","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_78","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#server_17","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_79","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#all-scenarios_26","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=bert-99.9 \\\n   --implementation=reference \\\n   --framework=tensorflow \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_80","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#nvidia-mlperf-implementation","title":"Nvidia MLPerf Implementation","text":"<p>BERT-99</p> edgedatacenter"},{"location":"benchmarks/language/bert/#edge-category_1","title":"Edge category","text":"<p>In the edge category, bert-99 has Offline, SingleStream scenarios and all the scenarios are mandatory for a closed division submission.</p> TensorRT"},{"location":"benchmarks/language/bert/#tensorrt-framework","title":"TensorRT framework","text":"CUDA"},{"location":"benchmarks/language/bert/#cuda-device_9","title":"CUDA device","text":""},{"location":"benchmarks/language/bert/#docker-setup-command_27","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=bert-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineSingleStreamAll Scenarios"},{"location":"benchmarks/language/bert/#offline_27","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=bert-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_81","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#singlestream_9","title":"# SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=bert-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_82","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#all-scenarios_27","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge  \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_83","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#datacenter-category_2","title":"Datacenter category","text":"<p>In the datacenter category, bert-99 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> TensorRT"},{"location":"benchmarks/language/bert/#tensorrt-framework_1","title":"TensorRT framework","text":"CUDA"},{"location":"benchmarks/language/bert/#cuda-device_10","title":"CUDA device","text":""},{"location":"benchmarks/language/bert/#docker-setup-command_28","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=bert-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios <p>BERT-99.9</p> datacenter"},{"location":"benchmarks/language/bert/#offline_28","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=bert-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_84","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#server_18","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=bert-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_85","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#all-scenarios_28","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_86","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#datacenter-category_3","title":"Datacenter category","text":"<p>In the datacenter category, bert-99.9 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> TensorRT"},{"location":"benchmarks/language/bert/#tensorrt-framework_2","title":"TensorRT framework","text":"CUDA"},{"location":"benchmarks/language/bert/#cuda-device_11","title":"CUDA device","text":""},{"location":"benchmarks/language/bert/#docker-setup-command_29","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=bert-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/language/bert/#offline_29","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=bert-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_87","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#server_19","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=bert-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_88","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#all-scenarios_29","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=bert-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_89","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#intel-mlperf-implementation","title":"Intel MLPerf Implementation","text":"<p>BERT-99</p> edgedatacenter"},{"location":"benchmarks/language/bert/#edge-category_2","title":"Edge category","text":"<p>In the edge category, bert-99 has Offline, SingleStream scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/language/bert/#pytorch-framework_3","title":"Pytorch framework","text":"CPU"},{"location":"benchmarks/language/bert/#cpu-device_9","title":"CPU device","text":""},{"location":"benchmarks/language/bert/#docker-setup-command_30","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=bert-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamAll Scenarios"},{"location":"benchmarks/language/bert/#offline_30","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=bert-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_90","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#singlestream_10","title":"# SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=bert-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_91","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#all-scenarios_30","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_92","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#datacenter-category_4","title":"Datacenter category","text":"<p>In the datacenter category, bert-99 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/language/bert/#pytorch-framework_4","title":"Pytorch framework","text":"CPU"},{"location":"benchmarks/language/bert/#cpu-device_10","title":"CPU device","text":""},{"location":"benchmarks/language/bert/#docker-setup-command_31","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=bert-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios <p>BERT-99.9</p> datacenter"},{"location":"benchmarks/language/bert/#offline_31","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=bert-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_93","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#server_20","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=bert-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_94","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#all-scenarios_31","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_95","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#datacenter-category_5","title":"Datacenter category","text":"<p>In the datacenter category, bert-99.9 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/language/bert/#pytorch-framework_5","title":"Pytorch framework","text":"CPU"},{"location":"benchmarks/language/bert/#cpu-device_11","title":"CPU device","text":""},{"location":"benchmarks/language/bert/#docker-setup-command_32","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=bert-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/language/bert/#offline_32","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=bert-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_96","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#server_21","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=bert-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_97","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#all-scenarios_32","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=bert-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_98","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#qualcomm-ai100-mlperf-implementation","title":"Qualcomm AI100 MLPerf Implementation","text":"<p>BERT-99</p> edgedatacenter"},{"location":"benchmarks/language/bert/#edge-category_3","title":"Edge category","text":"<p>In the edge category, bert-99 has Offline, SingleStream scenarios and all the scenarios are mandatory for a closed division submission.</p> Glow"},{"location":"benchmarks/language/bert/#glow-framework","title":"Glow framework","text":"QAIC"},{"location":"benchmarks/language/bert/#qaic-device","title":"QAIC device","text":""},{"location":"benchmarks/language/bert/#docker-setup-command_33","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=bert-99 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=qaic  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamAll Scenarios"},{"location":"benchmarks/language/bert/#offline_33","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=bert-99 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=qaic \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_99","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#singlestream_11","title":"# SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=bert-99 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution-mode=valid \\\n   --device=qaic \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_100","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#all-scenarios_33","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=edge  \\\n   --execution-mode=valid \\\n   --device=qaic \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_101","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#datacenter-category_6","title":"Datacenter category","text":"<p>In the datacenter category, bert-99 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Glow"},{"location":"benchmarks/language/bert/#glow-framework_1","title":"Glow framework","text":"QAIC"},{"location":"benchmarks/language/bert/#qaic-device_1","title":"QAIC device","text":""},{"location":"benchmarks/language/bert/#docker-setup-command_34","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=bert-99 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=qaic  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios <p>BERT-99.9</p> datacenter"},{"location":"benchmarks/language/bert/#offline_34","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=bert-99 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=qaic \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_102","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#server_22","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=bert-99 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=qaic \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_103","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#all-scenarios_34","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=bert-99 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=qaic \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_104","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#datacenter-category_7","title":"Datacenter category","text":"<p>In the datacenter category, bert-99.9 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Glow"},{"location":"benchmarks/language/bert/#glow-framework_2","title":"Glow framework","text":"QAIC"},{"location":"benchmarks/language/bert/#qaic-device_2","title":"QAIC device","text":""},{"location":"benchmarks/language/bert/#docker-setup-command_35","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=bert-99.9 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=qaic  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/language/bert/#offline_35","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=bert-99.9 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=qaic \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_105","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#server_23","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=bert-99.9 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=qaic \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_106","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/bert/#all-scenarios_35","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=bert-99.9 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=qaic \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/bert/#run-options_107","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/","title":"Text Summarization using GPT-J","text":""},{"location":"benchmarks/language/gpt-j/#dataset","title":"Dataset","text":"<p>The benchmark implementation run command will automatically download the validation and calibration datasets and do the necessary preprocessing. In case you want to download only the datasets, you can use the below commands.</p> Validation <p>GPT-J validation run uses the CNNDM dataset.</p>"},{"location":"benchmarks/language/gpt-j/#get-validation-dataset","title":"Get Validation Dataset","text":"<pre><code>cm run script --tags=get,dataset,cnndm,validation -j\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#model","title":"Model","text":"<p>The benchmark implementation run command will automatically download the required model and do the necessary conversions. In case you want to only download the official model, you can use the below commands.</p> <p>Get the Official MLPerf GPT-J Model</p> Pytorch"},{"location":"benchmarks/language/gpt-j/#pytorch","title":"Pytorch","text":"<pre><code>cm run script --tags=get,ml-model,gptj,_pytorch -j\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#benchmark-implementations","title":"Benchmark Implementations","text":"MLCommons-PythonNvidiaIntelQualcomm"},{"location":"benchmarks/language/gpt-j/#mlperf-reference-implementation-in-python","title":"MLPerf Reference Implementation in Python","text":"<p>GPT-J-99</p> edgedatacenter"},{"location":"benchmarks/language/gpt-j/#edge-category","title":"Edge category","text":"<p>In the edge category, gptj-99 has Offline, SingleStream scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/language/gpt-j/#pytorch-framework","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"benchmarks/language/gpt-j/#cpu-device","title":"CPU device","text":""},{"location":"benchmarks/language/gpt-j/#docker-setup-command","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamAll Scenarios"},{"location":"benchmarks/language/gpt-j/#offline","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#run-options","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#singlestream","title":"# SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#run-options_1","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#all-scenarios","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#run-options_2","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#cuda-device","title":"CUDA device","text":""},{"location":"benchmarks/language/gpt-j/#docker-setup-command_1","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineSingleStreamAll Scenarios"},{"location":"benchmarks/language/gpt-j/#offline_1","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#run-options_3","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#singlestream_1","title":"# SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#run-options_4","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#all-scenarios_1","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#run-options_5","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#rocm-device","title":"ROCm device","text":""},{"location":"benchmarks/language/gpt-j/#docker-setup-command_2","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=rocm  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamAll Scenarios"},{"location":"benchmarks/language/gpt-j/#offline_2","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#run-options_6","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#singlestream_2","title":"# SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#run-options_7","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#all-scenarios_2","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#run-options_8","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#datacenter-category","title":"Datacenter category","text":"<p>In the datacenter category, gptj-99 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/language/gpt-j/#pytorch-framework_1","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"benchmarks/language/gpt-j/#cpu-device_1","title":"CPU device","text":""},{"location":"benchmarks/language/gpt-j/#docker-setup-command_3","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/language/gpt-j/#offline_3","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#run-options_9","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#server","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#run-options_10","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#all-scenarios_3","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#run-options_11","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#cuda-device_1","title":"CUDA device","text":""},{"location":"benchmarks/language/gpt-j/#docker-setup-command_4","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/language/gpt-j/#offline_4","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#run-options_12","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#server_1","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#run-options_13","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#all-scenarios_4","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#run-options_14","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#rocm-device_1","title":"ROCm device","text":""},{"location":"benchmarks/language/gpt-j/#docker-setup-command_5","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=rocm  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios <p>GPTJ-99.9</p> edgedatacenter"},{"location":"benchmarks/language/gpt-j/#offline_5","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#run-options_15","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#server_2","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#run-options_16","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#all-scenarios_5","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=gptj-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#run-options_17","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#edge-category_1","title":"Edge category","text":"<p>In the edge category, gptj-99.9 has Offline, SingleStream scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/language/gpt-j/#pytorch-framework_2","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"benchmarks/language/gpt-j/#cpu-device_2","title":"CPU device","text":""},{"location":"benchmarks/language/gpt-j/#docker-setup-command_6","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamAll Scenarios"},{"location":"benchmarks/language/gpt-j/#offline_6","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#run-options_18","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#singlestream_3","title":"# SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#run-options_19","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#all-scenarios_6","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#run-options_20","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#cuda-device_2","title":"CUDA device","text":""},{"location":"benchmarks/language/gpt-j/#docker-setup-command_7","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineSingleStreamAll Scenarios"},{"location":"benchmarks/language/gpt-j/#offline_7","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#run-options_21","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#singlestream_4","title":"# SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#run-options_22","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#all-scenarios_7","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#run-options_23","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#rocm-device_2","title":"ROCm device","text":""},{"location":"benchmarks/language/gpt-j/#docker-setup-command_8","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=rocm  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamAll Scenarios"},{"location":"benchmarks/language/gpt-j/#offline_8","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#run-options_24","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#singlestream_5","title":"# SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#run-options_25","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#all-scenarios_8","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#run-options_26","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#datacenter-category_1","title":"Datacenter category","text":"<p>In the datacenter category, gptj-99.9 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/language/gpt-j/#pytorch-framework_3","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"benchmarks/language/gpt-j/#cpu-device_3","title":"CPU device","text":""},{"location":"benchmarks/language/gpt-j/#docker-setup-command_9","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/language/gpt-j/#offline_9","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#run-options_27","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#server_3","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#run-options_28","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#all-scenarios_9","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#run-options_29","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#cuda-device_3","title":"CUDA device","text":""},{"location":"benchmarks/language/gpt-j/#docker-setup-command_10","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/language/gpt-j/#offline_10","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#run-options_30","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#server_4","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#run-options_31","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#all-scenarios_10","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#run-options_32","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#rocm-device_3","title":"ROCm device","text":""},{"location":"benchmarks/language/gpt-j/#docker-setup-command_11","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=rocm  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/language/gpt-j/#offline_11","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#run-options_33","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#server_5","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#run-options_34","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#all-scenarios_11","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=gptj-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#run-options_35","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#nvidia-mlperf-implementation","title":"Nvidia MLPerf Implementation","text":"<p>GPTJ-99</p> edgedatacenter"},{"location":"benchmarks/language/gpt-j/#edge-category_2","title":"Edge category","text":"<p>In the edge category, gptj-99 has Offline, SingleStream scenarios and all the scenarios are mandatory for a closed division submission.</p> TensorRT"},{"location":"benchmarks/language/gpt-j/#tensorrt-framework","title":"TensorRT framework","text":"CUDA"},{"location":"benchmarks/language/gpt-j/#cuda-device_4","title":"CUDA device","text":""},{"location":"benchmarks/language/gpt-j/#docker-setup-command_12","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=gptj-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineSingleStreamAll Scenarios"},{"location":"benchmarks/language/gpt-j/#offline_12","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=gptj-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#run-options_36","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#singlestream_6","title":"# SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=gptj-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#run-options_37","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#all-scenarios_12","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=gptj-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge  \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#run-options_38","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#datacenter-category_2","title":"Datacenter category","text":"<p>In the datacenter category, gptj-99 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> TensorRT"},{"location":"benchmarks/language/gpt-j/#tensorrt-framework_1","title":"TensorRT framework","text":"CUDA"},{"location":"benchmarks/language/gpt-j/#cuda-device_5","title":"CUDA device","text":""},{"location":"benchmarks/language/gpt-j/#docker-setup-command_13","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=gptj-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios <p>GPTJ-99.9</p> edgedatacenter"},{"location":"benchmarks/language/gpt-j/#offline_13","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=gptj-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#run-options_39","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#server_6","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=gptj-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#run-options_40","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#all-scenarios_13","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=gptj-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#run-options_41","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#edge-category_3","title":"Edge category","text":"<p>In the edge category, gptj-99.9 has Offline, SingleStream scenarios and all the scenarios are mandatory for a closed division submission.</p> TensorRT"},{"location":"benchmarks/language/gpt-j/#tensorrt-framework_2","title":"TensorRT framework","text":"CUDA"},{"location":"benchmarks/language/gpt-j/#cuda-device_6","title":"CUDA device","text":""},{"location":"benchmarks/language/gpt-j/#docker-setup-command_14","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=gptj-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineSingleStreamAll Scenarios"},{"location":"benchmarks/language/gpt-j/#offline_14","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=gptj-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#run-options_42","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#singlestream_7","title":"# SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=gptj-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#run-options_43","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#all-scenarios_14","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=gptj-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge  \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#run-options_44","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#datacenter-category_3","title":"Datacenter category","text":"<p>In the datacenter category, gptj-99.9 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> TensorRT"},{"location":"benchmarks/language/gpt-j/#tensorrt-framework_3","title":"TensorRT framework","text":"CUDA"},{"location":"benchmarks/language/gpt-j/#cuda-device_7","title":"CUDA device","text":""},{"location":"benchmarks/language/gpt-j/#docker-setup-command_15","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=gptj-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/language/gpt-j/#offline_15","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=gptj-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#run-options_45","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#server_7","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=gptj-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#run-options_46","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#all-scenarios_15","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=gptj-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#run-options_47","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#intel-mlperf-implementation","title":"Intel MLPerf Implementation","text":"<p>GPTJ-99</p> edgedatacenter"},{"location":"benchmarks/language/gpt-j/#edge-category_4","title":"Edge category","text":"<p>In the edge category, gptj-99 has Offline, SingleStream scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/language/gpt-j/#pytorch-framework_4","title":"Pytorch framework","text":"CPU"},{"location":"benchmarks/language/gpt-j/#cpu-device_4","title":"CPU device","text":""},{"location":"benchmarks/language/gpt-j/#docker-setup-command_16","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=gptj-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamAll Scenarios"},{"location":"benchmarks/language/gpt-j/#offline_16","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=gptj-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#run-options_48","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#singlestream_8","title":"# SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=gptj-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#run-options_49","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#all-scenarios_16","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=gptj-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#run-options_50","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#datacenter-category_4","title":"Datacenter category","text":"<p>In the datacenter category, gptj-99 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/language/gpt-j/#pytorch-framework_5","title":"Pytorch framework","text":"CPU"},{"location":"benchmarks/language/gpt-j/#cpu-device_5","title":"CPU device","text":""},{"location":"benchmarks/language/gpt-j/#docker-setup-command_17","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=gptj-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/language/gpt-j/#offline_17","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=gptj-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#run-options_51","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#server_8","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=gptj-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#run-options_52","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#all-scenarios_17","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=gptj-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#run-options_53","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#qualcomm-ai100-mlperf-implementation","title":"Qualcomm AI100 MLPerf Implementation","text":"<p>GPTJ-99</p> edgedatacenter"},{"location":"benchmarks/language/gpt-j/#edge-category_5","title":"Edge category","text":"<p>In the edge category, gptj-99 has Offline, SingleStream scenarios and all the scenarios are mandatory for a closed division submission.</p> Glow"},{"location":"benchmarks/language/gpt-j/#glow-framework","title":"Glow framework","text":"QAIC"},{"location":"benchmarks/language/gpt-j/#qaic-device","title":"QAIC device","text":""},{"location":"benchmarks/language/gpt-j/#docker-setup-command_18","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=gptj-99 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=qaic  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamAll Scenarios"},{"location":"benchmarks/language/gpt-j/#offline_18","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=gptj-99 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=qaic \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#run-options_54","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#singlestream_9","title":"# SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=gptj-99 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution-mode=valid \\\n   --device=qaic \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#run-options_55","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#all-scenarios_18","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=gptj-99 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=edge  \\\n   --execution-mode=valid \\\n   --device=qaic \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#run-options_56","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#datacenter-category_5","title":"Datacenter category","text":"<p>In the datacenter category, gptj-99 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Glow"},{"location":"benchmarks/language/gpt-j/#glow-framework_1","title":"Glow framework","text":"QAIC"},{"location":"benchmarks/language/gpt-j/#qaic-device_1","title":"QAIC device","text":""},{"location":"benchmarks/language/gpt-j/#docker-setup-command_19","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=gptj-99 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=qaic  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/language/gpt-j/#offline_19","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=gptj-99 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=qaic \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#run-options_57","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#server_9","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=gptj-99 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=qaic \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#run-options_58","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/gpt-j/#all-scenarios_19","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=gptj-99 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=qaic \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/gpt-j/#run-options_59","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/llama2-70b/","title":"Text Summarization using LLAMA2-70b","text":""},{"location":"benchmarks/language/llama2-70b/#dataset","title":"Dataset","text":"<p>The benchmark implementation run command will automatically download the validation and calibration datasets and do the necessary preprocessing. In case you want to download only the datasets, you can use the below commands.</p> Validation <p>LLAMA2-70b validation run uses the Open ORCA dataset.</p>"},{"location":"benchmarks/language/llama2-70b/#get-validation-dataset","title":"Get Validation Dataset","text":"<pre><code>cm run script --tags=get,dataset,openorca,validation -j\n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#model","title":"Model","text":"<p>The benchmark implementation run command will automatically download the required model and do the necessary conversions. In case you want to only download the official model, you can use the below commands.</p> <p>Get the Official MLPerf LLAMA2-70b Model</p> Pytorch"},{"location":"benchmarks/language/llama2-70b/#pytorch","title":"Pytorch","text":"<pre><code>cm run script --tags=get,ml-model,llama2-70b,_pytorch -j\n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#benchmark-implementations","title":"Benchmark Implementations","text":"MLCommons-PythonNvidiaQualcomm"},{"location":"benchmarks/language/llama2-70b/#mlperf-reference-implementation-in-python","title":"MLPerf Reference Implementation in Python","text":"<p>LLAMA2-70b-99</p> datacenter"},{"location":"benchmarks/language/llama2-70b/#datacenter-category","title":"Datacenter category","text":"<p>In the datacenter category, llama2-70b-99 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/language/llama2-70b/#pytorch-framework","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"benchmarks/language/llama2-70b/#cpu-device","title":"CPU device","text":""},{"location":"benchmarks/language/llama2-70b/#docker-setup-command","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/language/llama2-70b/#offline","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#run-options","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/llama2-70b/#server","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#run-options_1","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/llama2-70b/#all-scenarios","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#run-options_2","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/llama2-70b/#cuda-device","title":"CUDA device","text":""},{"location":"benchmarks/language/llama2-70b/#docker-setup-command_1","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/language/llama2-70b/#offline_1","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#run-options_3","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/llama2-70b/#server_1","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#run-options_4","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/llama2-70b/#all-scenarios_1","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#run-options_5","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/llama2-70b/#rocm-device","title":"ROCm device","text":""},{"location":"benchmarks/language/llama2-70b/#docker-setup-command_2","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=rocm  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios <p>LLAMA2-70b-99.9</p> datacenter"},{"location":"benchmarks/language/llama2-70b/#offline_2","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#run-options_6","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/llama2-70b/#server_2","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#run-options_7","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/llama2-70b/#all-scenarios_2","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=llama2-70b-99 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#run-options_8","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/llama2-70b/#datacenter-category_1","title":"Datacenter category","text":"<p>In the datacenter category, llama2-70b-99.9 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/language/llama2-70b/#pytorch-framework_1","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"benchmarks/language/llama2-70b/#cpu-device_1","title":"CPU device","text":""},{"location":"benchmarks/language/llama2-70b/#docker-setup-command_3","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/language/llama2-70b/#offline_3","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#run-options_9","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/llama2-70b/#server_3","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#run-options_10","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/llama2-70b/#all-scenarios_3","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#run-options_11","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/llama2-70b/#cuda-device_1","title":"CUDA device","text":""},{"location":"benchmarks/language/llama2-70b/#docker-setup-command_4","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/language/llama2-70b/#offline_4","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#run-options_12","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/llama2-70b/#server_4","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#run-options_13","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/llama2-70b/#all-scenarios_4","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#run-options_14","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/llama2-70b/#rocm-device_1","title":"ROCm device","text":""},{"location":"benchmarks/language/llama2-70b/#docker-setup-command_5","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=rocm  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/language/llama2-70b/#offline_5","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#run-options_15","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/llama2-70b/#server_5","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#run-options_16","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/llama2-70b/#all-scenarios_5","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=llama2-70b-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#run-options_17","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/llama2-70b/#nvidia-mlperf-implementation","title":"Nvidia MLPerf Implementation","text":"<p>LLAMA2-70b-99</p> datacenter"},{"location":"benchmarks/language/llama2-70b/#datacenter-category_2","title":"Datacenter category","text":"<p>In the datacenter category, llama2-70b-99 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> TensorRT"},{"location":"benchmarks/language/llama2-70b/#tensorrt-framework","title":"TensorRT framework","text":"CUDA"},{"location":"benchmarks/language/llama2-70b/#cuda-device_2","title":"CUDA device","text":""},{"location":"benchmarks/language/llama2-70b/#docker-setup-command_6","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=llama2-70b-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios <p>LLAMA2-70b-99.9</p> datacenter"},{"location":"benchmarks/language/llama2-70b/#offline_6","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=llama2-70b-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#run-options_18","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/llama2-70b/#server_6","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=llama2-70b-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#run-options_19","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/llama2-70b/#all-scenarios_6","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=llama2-70b-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#run-options_20","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/llama2-70b/#datacenter-category_3","title":"Datacenter category","text":"<p>In the datacenter category, llama2-70b-99.9 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> TensorRT"},{"location":"benchmarks/language/llama2-70b/#tensorrt-framework_1","title":"TensorRT framework","text":"CUDA"},{"location":"benchmarks/language/llama2-70b/#cuda-device_3","title":"CUDA device","text":""},{"location":"benchmarks/language/llama2-70b/#docker-setup-command_7","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=llama2-70b-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/language/llama2-70b/#offline_7","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=llama2-70b-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#run-options_21","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/llama2-70b/#server_7","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=llama2-70b-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#run-options_22","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/llama2-70b/#all-scenarios_7","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=llama2-70b-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#run-options_23","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/llama2-70b/#qualcomm-ai100-mlperf-implementation","title":"Qualcomm AI100 MLPerf Implementation","text":"<p>LLAMA2-70b-99</p> datacenter"},{"location":"benchmarks/language/llama2-70b/#datacenter-category_4","title":"Datacenter category","text":"<p>In the datacenter category, llama2-70b-99 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Glow"},{"location":"benchmarks/language/llama2-70b/#glow-framework","title":"Glow framework","text":"QAIC"},{"location":"benchmarks/language/llama2-70b/#qaic-device","title":"QAIC device","text":""},{"location":"benchmarks/language/llama2-70b/#docker-setup-command_8","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=llama2-70b-99 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=qaic  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/language/llama2-70b/#offline_8","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=llama2-70b-99 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=qaic \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#run-options_24","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/llama2-70b/#server_8","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=llama2-70b-99 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=qaic \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#run-options_25","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/language/llama2-70b/#all-scenarios_8","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=llama2-70b-99 \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=qaic \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/language/llama2-70b/#run-options_26","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/","title":"Medical Imaging using 3d-unet (KiTS 2019 kidney tumor segmentation task)","text":""},{"location":"benchmarks/medical_imaging/3d-unet/#dataset","title":"Dataset","text":"<p>The benchmark implementation run command will automatically download the validation and calibration datasets and do the necessary preprocessing. In case you want to download only the datasets, you can use the below commands.</p> Validation <p>3d-unet validation run uses the KiTS19 dataset performing KiTS 2019 kidney tumor segmentation task</p>"},{"location":"benchmarks/medical_imaging/3d-unet/#get-validation-dataset","title":"Get Validation Dataset","text":"<pre><code>cm run script --tags=get,dataset,kits19,validation -j\n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#model","title":"Model","text":"<p>The benchmark implementation run command will automatically download the required model and do the necessary conversions. In case you want to only download the official model, you can use the below commands.</p> <p>Get the Official MLPerf 3d-unet Model</p> PytorchOnnxTensorflow"},{"location":"benchmarks/medical_imaging/3d-unet/#pytorch","title":"Pytorch","text":"<pre><code>cm run script --tags=get,ml-model,3d-unet,_pytorch -j\n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#onnx","title":"Onnx","text":"<pre><code>cm run script --tags=get,ml-model,3d-unet,_onnx -j\n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#tensorflow","title":"Tensorflow","text":"<pre><code>cm run script --tags=get,ml-model,3d-unet,_tensorflow -j\n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#benchmark-implementations","title":"Benchmark Implementations","text":"MLCommons-PythonNvidiaIntel"},{"location":"benchmarks/medical_imaging/3d-unet/#mlperf-reference-implementation-in-python","title":"MLPerf Reference Implementation in Python","text":"<p>3d-unet-99.9    </p> edgedatacenter"},{"location":"benchmarks/medical_imaging/3d-unet/#edge-category","title":"Edge category","text":"<p>In the edge category, 3d-unet-99.9 has Offline, SingleStream scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/medical_imaging/3d-unet/#pytorch-framework","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"benchmarks/medical_imaging/3d-unet/#cpu-device","title":"CPU device","text":""},{"location":"benchmarks/medical_imaging/3d-unet/#docker-setup-command","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamAll Scenarios"},{"location":"benchmarks/medical_imaging/3d-unet/#offline","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#run-options","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#singlestream","title":"# SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#run-options_1","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#all-scenarios","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#run-options_2","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#cuda-device","title":"CUDA device","text":""},{"location":"benchmarks/medical_imaging/3d-unet/#docker-setup-command_1","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineSingleStreamAll Scenarios"},{"location":"benchmarks/medical_imaging/3d-unet/#offline_1","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#run-options_3","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#singlestream_1","title":"# SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#run-options_4","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#all-scenarios_1","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#run-options_5","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#rocm-device","title":"ROCm device","text":""},{"location":"benchmarks/medical_imaging/3d-unet/#docker-setup-command_2","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=rocm  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamAll Scenarios"},{"location":"benchmarks/medical_imaging/3d-unet/#offline_2","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#run-options_6","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#singlestream_2","title":"# SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#run-options_7","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#all-scenarios_2","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#run-options_8","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#datacenter-category","title":"Datacenter category","text":"<p>In the datacenter category, 3d-unet-99.9 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/medical_imaging/3d-unet/#pytorch-framework_1","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"benchmarks/medical_imaging/3d-unet/#cpu-device_1","title":"CPU device","text":""},{"location":"benchmarks/medical_imaging/3d-unet/#docker-setup-command_3","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/medical_imaging/3d-unet/#offline_3","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#run-options_9","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#server","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#run-options_10","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#all-scenarios_3","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#run-options_11","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#cuda-device_1","title":"CUDA device","text":""},{"location":"benchmarks/medical_imaging/3d-unet/#docker-setup-command_4","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/medical_imaging/3d-unet/#offline_4","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#run-options_12","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#server_1","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#run-options_13","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#all-scenarios_4","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#run-options_14","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#rocm-device_1","title":"ROCm device","text":""},{"location":"benchmarks/medical_imaging/3d-unet/#docker-setup-command_5","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=rocm  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/medical_imaging/3d-unet/#offline_5","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#run-options_15","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#server_2","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#run-options_16","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#all-scenarios_5","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=3d-unet-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#run-options_17","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#nvidia-mlperf-implementation","title":"Nvidia MLPerf Implementation","text":"<p>3d-unet-99 </p> edgedatacenter"},{"location":"benchmarks/medical_imaging/3d-unet/#edge-category_1","title":"Edge category","text":"<p>In the edge category, 3d-unet-99 has Offline, SingleStream scenarios and all the scenarios are mandatory for a closed division submission.</p> TensorRT"},{"location":"benchmarks/medical_imaging/3d-unet/#tensorrt-framework","title":"TensorRT framework","text":"CUDA"},{"location":"benchmarks/medical_imaging/3d-unet/#cuda-device_2","title":"CUDA device","text":""},{"location":"benchmarks/medical_imaging/3d-unet/#docker-setup-command_6","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=3d-unet-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineSingleStreamAll Scenarios"},{"location":"benchmarks/medical_imaging/3d-unet/#offline_6","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=3d-unet-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#run-options_18","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#singlestream_3","title":"# SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=3d-unet-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#run-options_19","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#all-scenarios_6","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=3d-unet-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge  \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#run-options_20","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#datacenter-category_1","title":"Datacenter category","text":"<p>In the datacenter category, 3d-unet-99 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> TensorRT"},{"location":"benchmarks/medical_imaging/3d-unet/#tensorrt-framework_1","title":"TensorRT framework","text":"CUDA"},{"location":"benchmarks/medical_imaging/3d-unet/#cuda-device_3","title":"CUDA device","text":""},{"location":"benchmarks/medical_imaging/3d-unet/#docker-setup-command_7","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=3d-unet-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios <p>3d-unet-99.9 </p> edgedatacenter"},{"location":"benchmarks/medical_imaging/3d-unet/#offline_7","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=3d-unet-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#run-options_21","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#server_3","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=3d-unet-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#run-options_22","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#all-scenarios_7","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=3d-unet-99 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#run-options_23","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#edge-category_2","title":"Edge category","text":"<p>In the edge category, 3d-unet-99.9 has Offline, SingleStream scenarios and all the scenarios are mandatory for a closed division submission.</p> TensorRT"},{"location":"benchmarks/medical_imaging/3d-unet/#tensorrt-framework_2","title":"TensorRT framework","text":"CUDA"},{"location":"benchmarks/medical_imaging/3d-unet/#cuda-device_4","title":"CUDA device","text":""},{"location":"benchmarks/medical_imaging/3d-unet/#docker-setup-command_8","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=3d-unet-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineSingleStreamAll Scenarios"},{"location":"benchmarks/medical_imaging/3d-unet/#offline_8","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=3d-unet-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#run-options_24","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#singlestream_4","title":"# SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=3d-unet-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#run-options_25","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#all-scenarios_8","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=3d-unet-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge  \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#run-options_26","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#datacenter-category_2","title":"Datacenter category","text":"<p>In the datacenter category, 3d-unet-99.9 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> TensorRT"},{"location":"benchmarks/medical_imaging/3d-unet/#tensorrt-framework_3","title":"TensorRT framework","text":"CUDA"},{"location":"benchmarks/medical_imaging/3d-unet/#cuda-device_5","title":"CUDA device","text":""},{"location":"benchmarks/medical_imaging/3d-unet/#docker-setup-command_9","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=3d-unet-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/medical_imaging/3d-unet/#offline_9","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=3d-unet-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#run-options_27","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#server_4","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=3d-unet-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#run-options_28","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#all-scenarios_9","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=3d-unet-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#run-options_29","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#intel-mlperf-implementation","title":"Intel MLPerf Implementation","text":"<p>3d-unet-99</p> edgedatacenter"},{"location":"benchmarks/medical_imaging/3d-unet/#edge-category_3","title":"Edge category","text":"<p>In the edge category, 3d-unet-99 has Offline, SingleStream scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/medical_imaging/3d-unet/#pytorch-framework_2","title":"Pytorch framework","text":"CPU"},{"location":"benchmarks/medical_imaging/3d-unet/#cpu-device_2","title":"CPU device","text":""},{"location":"benchmarks/medical_imaging/3d-unet/#docker-setup-command_10","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=3d-unet-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamAll Scenarios"},{"location":"benchmarks/medical_imaging/3d-unet/#offline_10","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=3d-unet-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#run-options_30","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#singlestream_5","title":"# SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=3d-unet-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#run-options_31","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#all-scenarios_10","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=3d-unet-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#run-options_32","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#datacenter-category_3","title":"Datacenter category","text":"<p>In the datacenter category, 3d-unet-99 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/medical_imaging/3d-unet/#pytorch-framework_3","title":"Pytorch framework","text":"CPU"},{"location":"benchmarks/medical_imaging/3d-unet/#cpu-device_3","title":"CPU device","text":""},{"location":"benchmarks/medical_imaging/3d-unet/#docker-setup-command_11","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=3d-unet-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios <p>3d-unet-99.9</p> edgedatacenter"},{"location":"benchmarks/medical_imaging/3d-unet/#offline_11","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=3d-unet-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#run-options_33","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#server_5","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=3d-unet-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#run-options_34","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#all-scenarios_11","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=3d-unet-99 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#run-options_35","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#edge-category_4","title":"Edge category","text":"<p>In the edge category, 3d-unet-99.9 has Offline, SingleStream scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/medical_imaging/3d-unet/#pytorch-framework_4","title":"Pytorch framework","text":"CPU"},{"location":"benchmarks/medical_imaging/3d-unet/#cpu-device_4","title":"CPU device","text":""},{"location":"benchmarks/medical_imaging/3d-unet/#docker-setup-command_12","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=3d-unet-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamAll Scenarios"},{"location":"benchmarks/medical_imaging/3d-unet/#offline_12","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=3d-unet-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#run-options_36","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#singlestream_6","title":"# SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=3d-unet-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#run-options_37","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#all-scenarios_12","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=3d-unet-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#run-options_38","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#datacenter-category_4","title":"Datacenter category","text":"<p>In the datacenter category, 3d-unet-99.9 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/medical_imaging/3d-unet/#pytorch-framework_5","title":"Pytorch framework","text":"CPU"},{"location":"benchmarks/medical_imaging/3d-unet/#cpu-device_5","title":"CPU device","text":""},{"location":"benchmarks/medical_imaging/3d-unet/#docker-setup-command_13","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=3d-unet-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/medical_imaging/3d-unet/#offline_13","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=3d-unet-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#run-options_39","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#server_6","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=3d-unet-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#run-options_40","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/medical_imaging/3d-unet/#all-scenarios_13","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=3d-unet-99.9 \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/medical_imaging/3d-unet/#run-options_41","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/","title":"Object Detection using Retinanet","text":""},{"location":"benchmarks/object_detection/retinanet/#dataset","title":"Dataset","text":"<p>The benchmark implementation run command will automatically download the validation and calibration datasets and do the necessary preprocessing. In case you want to download only the datasets, you can use the below commands.</p> ValidationCalibration <p>Retinanet validation run uses the OpenImages v6 MLPerf validation dataset resized to 800x800 and consisting of 24,576 images.</p> <p>Retinanet calibration dataset consist of 500 images selected from the OpenImages v6 dataset.</p> <pre><code>cm run script --tags=get,dataset,openimages,_calibration -j\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#get-validation-dataset","title":"Get Validation Dataset","text":"<pre><code>cm run script --tags=get,dataset,openimages,_validation -j\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#model","title":"Model","text":"<p>The benchmark implementation run command will automatically download the required model and do the necessary conversions. In case you want to only download the official model, you can use the below commands.</p> <p>Get the Official MLPerf Retinanet Model</p> PytorchOnnx"},{"location":"benchmarks/object_detection/retinanet/#pytorch","title":"Pytorch","text":"<pre><code>cm run script --tags=get,ml-model,retinanet,_pytorch -j\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#onnx","title":"Onnx","text":"<pre><code>cm run script --tags=get,ml-model,retinanet,_onnx -j\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#benchmark-implementations","title":"Benchmark Implementations","text":"MLCommons-PythonNvidiaIntelQualcommMLCommon-C++"},{"location":"benchmarks/object_detection/retinanet/#mlperf-reference-implementation-in-python","title":"MLPerf Reference Implementation in Python","text":"edgedatacenter"},{"location":"benchmarks/object_detection/retinanet/#edge-category","title":"Edge category","text":"<p>In the edge category, retinanet has Offline, SingleStream, Multistream scenarios and all the scenarios are mandatory for a closed division submission.</p> OnnxruntimePytorch"},{"location":"benchmarks/object_detection/retinanet/#onnxruntime-framework","title":"Onnxruntime framework","text":"CPUCUDAROCm"},{"location":"benchmarks/object_detection/retinanet/#cpu-device","title":"CPU device","text":""},{"location":"benchmarks/object_detection/retinanet/#docker-setup-command","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamMultistreamAll Scenarios"},{"location":"benchmarks/object_detection/retinanet/#offline","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#singlestream","title":"# SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_1","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#multistream","title":"# Multistream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Multistream \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_2","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge  \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_3","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#cuda-device","title":"CUDA device","text":""},{"location":"benchmarks/object_detection/retinanet/#docker-setup-command_1","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineSingleStreamMultistreamAll Scenarios"},{"location":"benchmarks/object_detection/retinanet/#offline_1","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_4","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#singlestream_1","title":"# SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_5","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#multistream_1","title":"# Multistream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Multistream \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_6","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_1","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge  \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_7","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#rocm-device","title":"ROCm device","text":""},{"location":"benchmarks/object_detection/retinanet/#docker-setup-command_2","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=rocm  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamMultistreamAll Scenarios"},{"location":"benchmarks/object_detection/retinanet/#offline_2","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_8","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#singlestream_2","title":"# SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_9","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#multistream_2","title":"# Multistream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Multistream \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_10","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_2","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=edge  \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_11","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#pytorch-framework","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"benchmarks/object_detection/retinanet/#cpu-device_1","title":"CPU device","text":""},{"location":"benchmarks/object_detection/retinanet/#docker-setup-command_3","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamMultistreamAll Scenarios"},{"location":"benchmarks/object_detection/retinanet/#offline_3","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_12","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#singlestream_3","title":"# SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_13","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#multistream_3","title":"# Multistream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Multistream \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_14","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_3","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_15","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#cuda-device_1","title":"CUDA device","text":""},{"location":"benchmarks/object_detection/retinanet/#docker-setup-command_4","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineSingleStreamMultistreamAll Scenarios"},{"location":"benchmarks/object_detection/retinanet/#offline_4","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_16","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#singlestream_4","title":"# SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_17","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#multistream_4","title":"# Multistream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Multistream \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_18","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_4","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_19","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#rocm-device_1","title":"ROCm device","text":""},{"location":"benchmarks/object_detection/retinanet/#docker-setup-command_5","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=rocm  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamMultistreamAll Scenarios"},{"location":"benchmarks/object_detection/retinanet/#offline_5","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_20","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#singlestream_5","title":"# SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_21","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#multistream_5","title":"# Multistream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Multistream \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_22","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_5","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_23","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#datacenter-category","title":"Datacenter category","text":"<p>In the datacenter category, retinanet has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> OnnxruntimePytorch"},{"location":"benchmarks/object_detection/retinanet/#onnxruntime-framework_1","title":"Onnxruntime framework","text":"CPUCUDAROCm"},{"location":"benchmarks/object_detection/retinanet/#cpu-device_2","title":"CPU device","text":""},{"location":"benchmarks/object_detection/retinanet/#docker-setup-command_6","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/object_detection/retinanet/#offline_6","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_24","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#server","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_25","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_6","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_26","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#cuda-device_2","title":"CUDA device","text":""},{"location":"benchmarks/object_detection/retinanet/#docker-setup-command_7","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/object_detection/retinanet/#offline_7","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_27","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#server_1","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_28","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_7","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_29","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#rocm-device_2","title":"ROCm device","text":""},{"location":"benchmarks/object_detection/retinanet/#docker-setup-command_8","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=rocm  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/object_detection/retinanet/#offline_8","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_30","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#server_2","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_31","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_8","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=onnxruntime \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_32","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#pytorch-framework_1","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"benchmarks/object_detection/retinanet/#cpu-device_3","title":"CPU device","text":""},{"location":"benchmarks/object_detection/retinanet/#docker-setup-command_9","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/object_detection/retinanet/#offline_9","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_33","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#server_3","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_34","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_9","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_35","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#cuda-device_3","title":"CUDA device","text":""},{"location":"benchmarks/object_detection/retinanet/#docker-setup-command_10","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/object_detection/retinanet/#offline_10","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_36","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#server_4","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_37","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_10","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_38","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#rocm-device_3","title":"ROCm device","text":""},{"location":"benchmarks/object_detection/retinanet/#docker-setup-command_11","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=rocm  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/object_detection/retinanet/#offline_11","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_39","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#server_5","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_40","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_11","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_41","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#nvidia-mlperf-implementation","title":"Nvidia MLPerf Implementation","text":"edgedatacenter"},{"location":"benchmarks/object_detection/retinanet/#edge-category_1","title":"Edge category","text":"<p>In the edge category, retinanet has Offline, SingleStream, Multistream scenarios and all the scenarios are mandatory for a closed division submission.</p> TensorRT"},{"location":"benchmarks/object_detection/retinanet/#tensorrt-framework","title":"TensorRT framework","text":"CUDA"},{"location":"benchmarks/object_detection/retinanet/#cuda-device_4","title":"CUDA device","text":""},{"location":"benchmarks/object_detection/retinanet/#docker-setup-command_12","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=retinanet \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineSingleStreamMultistreamAll Scenarios"},{"location":"benchmarks/object_detection/retinanet/#offline_12","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_42","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#singlestream_6","title":"# SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_43","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#multistream_6","title":"# Multistream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Multistream \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_44","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_12","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge  \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_45","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#datacenter-category_1","title":"Datacenter category","text":"<p>In the datacenter category, retinanet has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> TensorRT"},{"location":"benchmarks/object_detection/retinanet/#tensorrt-framework_1","title":"TensorRT framework","text":"CUDA"},{"location":"benchmarks/object_detection/retinanet/#cuda-device_5","title":"CUDA device","text":""},{"location":"benchmarks/object_detection/retinanet/#docker-setup-command_13","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=retinanet \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/object_detection/retinanet/#offline_13","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_46","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#server_6","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_47","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_13","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_48","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#intel-mlperf-implementation","title":"Intel MLPerf Implementation","text":"edgedatacenter"},{"location":"benchmarks/object_detection/retinanet/#edge-category_2","title":"Edge category","text":"<p>In the edge category, retinanet has Offline, SingleStream, Multistream scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/object_detection/retinanet/#pytorch-framework_2","title":"Pytorch framework","text":"CPU"},{"location":"benchmarks/object_detection/retinanet/#cpu-device_4","title":"CPU device","text":""},{"location":"benchmarks/object_detection/retinanet/#docker-setup-command_14","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=retinanet \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamMultistreamAll Scenarios"},{"location":"benchmarks/object_detection/retinanet/#offline_14","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_49","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#singlestream_7","title":"# SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_50","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#multistream_7","title":"# Multistream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Multistream \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_51","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_14","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_52","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#datacenter-category_2","title":"Datacenter category","text":"<p>In the datacenter category, retinanet has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/object_detection/retinanet/#pytorch-framework_3","title":"Pytorch framework","text":"CPU"},{"location":"benchmarks/object_detection/retinanet/#cpu-device_5","title":"CPU device","text":""},{"location":"benchmarks/object_detection/retinanet/#docker-setup-command_15","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=retinanet \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/object_detection/retinanet/#offline_15","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_53","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#server_7","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_54","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_15","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_55","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#qualcomm-ai100-mlperf-implementation","title":"Qualcomm AI100 MLPerf Implementation","text":"edgedatacenter"},{"location":"benchmarks/object_detection/retinanet/#edge-category_3","title":"Edge category","text":"<p>In the edge category, retinanet has Offline, SingleStream, Multistream scenarios and all the scenarios are mandatory for a closed division submission.</p> Glow"},{"location":"benchmarks/object_detection/retinanet/#glow-framework","title":"Glow framework","text":"QAIC"},{"location":"benchmarks/object_detection/retinanet/#qaic-device","title":"QAIC device","text":""},{"location":"benchmarks/object_detection/retinanet/#docker-setup-command_16","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=retinanet \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=qaic  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamMultistreamAll Scenarios"},{"location":"benchmarks/object_detection/retinanet/#offline_16","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=qaic \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_56","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#singlestream_8","title":"# SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution-mode=valid \\\n   --device=qaic \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_57","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#multistream_8","title":"# Multistream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=edge \\\n   --scenario=Multistream \\\n   --execution-mode=valid \\\n   --device=qaic \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_58","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_16","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=edge  \\\n   --execution-mode=valid \\\n   --device=qaic \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_59","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#datacenter-category_3","title":"Datacenter category","text":"<p>In the datacenter category, retinanet has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Glow"},{"location":"benchmarks/object_detection/retinanet/#glow-framework_1","title":"Glow framework","text":"QAIC"},{"location":"benchmarks/object_detection/retinanet/#qaic-device_1","title":"QAIC device","text":""},{"location":"benchmarks/object_detection/retinanet/#docker-setup-command_17","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=retinanet \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=qaic  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/object_detection/retinanet/#offline_17","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=qaic \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_60","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#server_8","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=qaic \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_61","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_17","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=qaic \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_62","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#mlperf-modular-implementation-in-c","title":"MLPerf Modular Implementation in C++","text":"edgedatacenter"},{"location":"benchmarks/object_detection/retinanet/#edge-category_4","title":"Edge category","text":"<p>In the edge category, retinanet has Offline, SingleStream, Multistream scenarios and all the scenarios are mandatory for a closed division submission.</p> Onnxruntime"},{"location":"benchmarks/object_detection/retinanet/#onnxruntime-framework_2","title":"Onnxruntime framework","text":"CPUCUDA"},{"location":"benchmarks/object_detection/retinanet/#cpu-device_6","title":"CPU device","text":""},{"location":"benchmarks/object_detection/retinanet/#docker-setup-command_18","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamMultistreamAll Scenarios"},{"location":"benchmarks/object_detection/retinanet/#offline_18","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_63","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#singlestream_9","title":"# SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_64","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#multistream_9","title":"# Multistream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Multistream \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_65","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_18","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge  \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_66","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#cuda-device_6","title":"CUDA device","text":""},{"location":"benchmarks/object_detection/retinanet/#docker-setup-command_19","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineSingleStreamMultistreamAll Scenarios"},{"location":"benchmarks/object_detection/retinanet/#offline_19","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_67","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#singlestream_10","title":"# SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_68","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#multistream_10","title":"# Multistream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge \\\n   --scenario=Multistream \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_69","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_19","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=edge  \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_70","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#datacenter-category_4","title":"Datacenter category","text":"<p>In the datacenter category, retinanet has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Onnxruntime"},{"location":"benchmarks/object_detection/retinanet/#onnxruntime-framework_3","title":"Onnxruntime framework","text":"CPUCUDA"},{"location":"benchmarks/object_detection/retinanet/#cpu-device_7","title":"CPU device","text":""},{"location":"benchmarks/object_detection/retinanet/#docker-setup-command_20","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/object_detection/retinanet/#offline_20","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_71","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#server_9","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_72","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_20","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_73","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#cuda-device_7","title":"CUDA device","text":""},{"location":"benchmarks/object_detection/retinanet/#docker-setup-command_21","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/object_detection/retinanet/#offline_21","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_74","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#server_10","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_75","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/object_detection/retinanet/#all-scenarios_21","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=retinanet \\\n   --implementation=cpp \\\n   --framework=onnxruntime \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/object_detection/retinanet/#run-options_76","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/","title":"Recommendation using DLRM v2","text":""},{"location":"benchmarks/recommendation/dlrm-v2/#dataset","title":"Dataset","text":"<p>The benchmark implementation run command will automatically download the validation and calibration datasets and do the necessary preprocessing. In case you want to download only the datasets, you can use the below commands.</p> Validation <p>DLRM validation run uses the Criteo dataset (Day 23).</p>"},{"location":"benchmarks/recommendation/dlrm-v2/#get-validation-dataset","title":"Get Validation Dataset","text":"<pre><code>cm run script --tags=get,dataset,criteo,validation -j\n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#model","title":"Model","text":"<p>The benchmark implementation run command will automatically download the required model and do the necessary conversions. In case you want to only download the official model, you can use the below commands.</p> <p>Get the Official MLPerf DLRM v2 Model</p> Pytorch"},{"location":"benchmarks/recommendation/dlrm-v2/#pytorch","title":"Pytorch","text":"<pre><code>cm run script --tags=get,ml-model,dlrm_v2,_pytorch -j\n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#benchmark-implementations","title":"Benchmark Implementations","text":"MLCommons-PythonNvidia"},{"location":"benchmarks/recommendation/dlrm-v2/#mlperf-reference-implementation-in-python","title":"MLPerf Reference Implementation in Python","text":"datacenter"},{"location":"benchmarks/recommendation/dlrm-v2/#datacenter-category","title":"Datacenter category","text":"<p>In the datacenter category, dlrm_v2-99.9 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/recommendation/dlrm-v2/#pytorch-framework","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"benchmarks/recommendation/dlrm-v2/#cpu-device","title":"CPU device","text":""},{"location":"benchmarks/recommendation/dlrm-v2/#docker-setup-command","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=dlrm_v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/recommendation/dlrm-v2/#offline","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=dlrm_v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#run-options","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#server","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=dlrm_v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#run-options_1","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#all-scenarios","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=dlrm_v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#run-options_2","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#cuda-device","title":"CUDA device","text":""},{"location":"benchmarks/recommendation/dlrm-v2/#docker-setup-command_1","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=dlrm_v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/recommendation/dlrm-v2/#offline_1","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=dlrm_v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#run-options_3","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#server_1","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=dlrm_v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#run-options_4","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#all-scenarios_1","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=dlrm_v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#run-options_5","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#rocm-device","title":"ROCm device","text":""},{"location":"benchmarks/recommendation/dlrm-v2/#docker-setup-command_2","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=dlrm_v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=rocm  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/recommendation/dlrm-v2/#offline_2","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=dlrm_v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#run-options_6","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#server_2","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=dlrm_v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#run-options_7","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#all-scenarios_2","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=dlrm_v2-99.9 \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#run-options_8","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#nvidia-mlperf-implementation","title":"Nvidia MLPerf Implementation","text":"datacenter"},{"location":"benchmarks/recommendation/dlrm-v2/#datacenter-category_1","title":"Datacenter category","text":"<p>In the datacenter category, dlrm_v2-99.9 has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> TensorRT"},{"location":"benchmarks/recommendation/dlrm-v2/#tensorrt-framework","title":"TensorRT framework","text":"CUDA"},{"location":"benchmarks/recommendation/dlrm-v2/#cuda-device_1","title":"CUDA device","text":""},{"location":"benchmarks/recommendation/dlrm-v2/#docker-setup-command_3","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=dlrm_v2-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/recommendation/dlrm-v2/#offline_3","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=dlrm_v2-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#run-options_9","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#server_3","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=dlrm_v2-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#run-options_10","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/recommendation/dlrm-v2/#all-scenarios_3","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=dlrm_v2-99.9 \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/recommendation/dlrm-v2/#run-options_11","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/","title":"Text to Image using Stable Diffusion","text":""},{"location":"benchmarks/text_to_image/sdxl/#dataset","title":"Dataset","text":"<p>The benchmark implementation run command will automatically download the validation and calibration datasets and do the necessary preprocessing. In case you want to download only the datasets, you can use the below commands.</p> Validation <p>Stable Diffusion validation run uses the Coco 2014 dataset.</p>"},{"location":"benchmarks/text_to_image/sdxl/#get-validation-dataset","title":"Get Validation Dataset","text":"<pre><code>cm run script --tags=get,dataset,coco2014,_validation -j\n</code></pre>"},{"location":"benchmarks/text_to_image/sdxl/#model","title":"Model","text":"<p>The benchmark implementation run command will automatically download the required model and do the necessary conversions. In case you want to only download the official model, you can use the below commands.</p> <p>Get the Official MLPerf Stable Diffusion Model</p> Pytorch"},{"location":"benchmarks/text_to_image/sdxl/#pytorch","title":"Pytorch","text":"<pre><code>cm run script --tags=get,ml-model,sdxl,_pytorch -j\n</code></pre>"},{"location":"benchmarks/text_to_image/sdxl/#benchmark-implementations","title":"Benchmark Implementations","text":"MLCommons-PythonNvidiaIntelQualcomm"},{"location":"benchmarks/text_to_image/sdxl/#mlperf-reference-implementation-in-python","title":"MLPerf Reference Implementation in Python","text":"edgedatacenter"},{"location":"benchmarks/text_to_image/sdxl/#edge-category","title":"Edge category","text":"<p>In the edge category, sdxl has Offline, SingleStream scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/text_to_image/sdxl/#pytorch-framework","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"benchmarks/text_to_image/sdxl/#cpu-device","title":"CPU device","text":""},{"location":"benchmarks/text_to_image/sdxl/#docker-setup-command","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamAll Scenarios"},{"location":"benchmarks/text_to_image/sdxl/#offline","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/text_to_image/sdxl/#run-options","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#singlestream","title":"# SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/text_to_image/sdxl/#run-options_1","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#all-scenarios","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/text_to_image/sdxl/#run-options_2","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#cuda-device","title":"CUDA device","text":""},{"location":"benchmarks/text_to_image/sdxl/#docker-setup-command_1","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineSingleStreamAll Scenarios"},{"location":"benchmarks/text_to_image/sdxl/#offline_1","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/text_to_image/sdxl/#run-options_3","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#singlestream_1","title":"# SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/text_to_image/sdxl/#run-options_4","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#all-scenarios_1","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/text_to_image/sdxl/#run-options_5","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#rocm-device","title":"ROCm device","text":""},{"location":"benchmarks/text_to_image/sdxl/#docker-setup-command_2","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=rocm  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamAll Scenarios"},{"location":"benchmarks/text_to_image/sdxl/#offline_2","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/text_to_image/sdxl/#run-options_6","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#singlestream_2","title":"# SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/text_to_image/sdxl/#run-options_7","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#all-scenarios_2","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/text_to_image/sdxl/#run-options_8","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#datacenter-category","title":"Datacenter category","text":"<p>In the datacenter category, sdxl has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/text_to_image/sdxl/#pytorch-framework_1","title":"Pytorch framework","text":"CPUCUDAROCm"},{"location":"benchmarks/text_to_image/sdxl/#cpu-device_1","title":"CPU device","text":""},{"location":"benchmarks/text_to_image/sdxl/#docker-setup-command_3","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/text_to_image/sdxl/#offline_3","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/text_to_image/sdxl/#run-options_9","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#server","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/text_to_image/sdxl/#run-options_10","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#all-scenarios_3","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/text_to_image/sdxl/#run-options_11","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#cuda-device_1","title":"CUDA device","text":""},{"location":"benchmarks/text_to_image/sdxl/#docker-setup-command_4","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/text_to_image/sdxl/#offline_4","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/text_to_image/sdxl/#run-options_12","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#server_1","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/text_to_image/sdxl/#run-options_13","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#all-scenarios_4","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/text_to_image/sdxl/#run-options_14","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#rocm-device_1","title":"ROCm device","text":""},{"location":"benchmarks/text_to_image/sdxl/#docker-setup-command_5","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=rocm  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/text_to_image/sdxl/#offline_5","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/text_to_image/sdxl/#run-options_15","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#server_2","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/text_to_image/sdxl/#run-options_16","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#all-scenarios_5","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=sdxl \\\n   --implementation=reference \\\n   --framework=pytorch \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=rocm \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/text_to_image/sdxl/#run-options_17","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#nvidia-mlperf-implementation","title":"Nvidia MLPerf Implementation","text":"edgedatacenter"},{"location":"benchmarks/text_to_image/sdxl/#edge-category_1","title":"Edge category","text":"<p>In the edge category, sdxl has Offline, SingleStream scenarios and all the scenarios are mandatory for a closed division submission.</p> TensorRT"},{"location":"benchmarks/text_to_image/sdxl/#tensorrt-framework","title":"TensorRT framework","text":"CUDA"},{"location":"benchmarks/text_to_image/sdxl/#cuda-device_2","title":"CUDA device","text":""},{"location":"benchmarks/text_to_image/sdxl/#docker-setup-command_6","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=sdxl \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineSingleStreamAll Scenarios"},{"location":"benchmarks/text_to_image/sdxl/#offline_6","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=sdxl \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/text_to_image/sdxl/#run-options_18","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#singlestream_3","title":"# SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=sdxl \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/text_to_image/sdxl/#run-options_19","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#all-scenarios_6","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=sdxl \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=edge  \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/text_to_image/sdxl/#run-options_20","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#datacenter-category_1","title":"Datacenter category","text":"<p>In the datacenter category, sdxl has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> TensorRT"},{"location":"benchmarks/text_to_image/sdxl/#tensorrt-framework_1","title":"TensorRT framework","text":"CUDA"},{"location":"benchmarks/text_to_image/sdxl/#cuda-device_3","title":"CUDA device","text":""},{"location":"benchmarks/text_to_image/sdxl/#docker-setup-command_7","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=sdxl \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cuda  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build </p> </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/text_to_image/sdxl/#offline_7","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=sdxl \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/text_to_image/sdxl/#run-options_21","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#server_3","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=sdxl \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/text_to_image/sdxl/#run-options_22","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#all-scenarios_7","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=sdxl \\\n   --implementation=nvidia \\\n   --framework=tensorrt \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=cuda \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/text_to_image/sdxl/#run-options_23","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#intel-mlperf-implementation","title":"Intel MLPerf Implementation","text":"<p>GPTJ-99</p> edgedatacenter"},{"location":"benchmarks/text_to_image/sdxl/#edge-category_2","title":"Edge category","text":"<p>In the edge category, sdxl has Offline, SingleStream scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/text_to_image/sdxl/#pytorch-framework_2","title":"Pytorch framework","text":"CPU"},{"location":"benchmarks/text_to_image/sdxl/#cpu-device_2","title":"CPU device","text":""},{"location":"benchmarks/text_to_image/sdxl/#docker-setup-command_8","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=sdxl \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamAll Scenarios"},{"location":"benchmarks/text_to_image/sdxl/#offline_8","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=sdxl \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/text_to_image/sdxl/#run-options_24","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#singlestream_4","title":"# SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=sdxl \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/text_to_image/sdxl/#run-options_25","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#all-scenarios_8","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=sdxl \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=edge  \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/text_to_image/sdxl/#run-options_26","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#datacenter-category_2","title":"Datacenter category","text":"<p>In the datacenter category, sdxl has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Pytorch"},{"location":"benchmarks/text_to_image/sdxl/#pytorch-framework_3","title":"Pytorch framework","text":"CPU"},{"location":"benchmarks/text_to_image/sdxl/#cpu-device_3","title":"CPU device","text":""},{"location":"benchmarks/text_to_image/sdxl/#docker-setup-command_9","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=sdxl \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=cpu  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/text_to_image/sdxl/#offline_9","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=sdxl \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/text_to_image/sdxl/#run-options_27","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#server_4","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=sdxl \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/text_to_image/sdxl/#run-options_28","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#all-scenarios_9","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=sdxl \\\n   --implementation=intel \\\n   --framework=pytorch \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=cpu \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/text_to_image/sdxl/#run-options_29","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#qualcomm-ai100-mlperf-implementation","title":"Qualcomm AI100 MLPerf Implementation","text":"<p>GPTJ-99</p> edgedatacenter"},{"location":"benchmarks/text_to_image/sdxl/#edge-category_3","title":"Edge category","text":"<p>In the edge category, sdxl has Offline, SingleStream scenarios and all the scenarios are mandatory for a closed division submission.</p> Glow"},{"location":"benchmarks/text_to_image/sdxl/#glow-framework","title":"Glow framework","text":"QAIC"},{"location":"benchmarks/text_to_image/sdxl/#qaic-device","title":"QAIC device","text":""},{"location":"benchmarks/text_to_image/sdxl/#docker-setup-command_10","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=sdxl \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=qaic  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineSingleStreamAll Scenarios"},{"location":"benchmarks/text_to_image/sdxl/#offline_10","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=sdxl \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=edge \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=qaic \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/text_to_image/sdxl/#run-options_30","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#singlestream_5","title":"# SingleStream","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=sdxl \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=edge \\\n   --scenario=SingleStream \\\n   --execution-mode=valid \\\n   --device=qaic \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/text_to_image/sdxl/#run-options_31","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#all-scenarios_10","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=sdxl \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=edge  \\\n   --execution-mode=valid \\\n   --device=qaic \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/text_to_image/sdxl/#run-options_32","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#datacenter-category_3","title":"Datacenter category","text":"<p>In the datacenter category, sdxl has Offline, Server scenarios and all the scenarios are mandatory for a closed division submission.</p> Glow"},{"location":"benchmarks/text_to_image/sdxl/#glow-framework_1","title":"Glow framework","text":"QAIC"},{"location":"benchmarks/text_to_image/sdxl/#qaic-device_1","title":"QAIC device","text":""},{"location":"benchmarks/text_to_image/sdxl/#docker-setup-command_11","title":"Docker Setup Command","text":"<p><pre><code>cm run script --tags=run-mlperf,inference,_find-performance,_full \\\n   --model=sdxl \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=test \\\n   --device=qaic  \\\n   --docker --quiet \\\n   --test_query_count=100\n</code></pre> The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p> <p>  Please click here to see more options for the docker launch  <ul> <li> <p><code>--docker_cm_repo &lt;Custom CM repo URL&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p> </li> <li> <p><code>--docker_cache=no</code>: to not use docker cache during the image build</p> </li> <li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li> <li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL </li> </ul> OfflineServerAll Scenarios"},{"location":"benchmarks/text_to_image/sdxl/#offline_11","title":"# Offline","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=sdxl \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=datacenter \\\n   --scenario=Offline \\\n   --execution-mode=valid \\\n   --device=qaic \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/text_to_image/sdxl/#run-options_33","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#server_5","title":"# Server","text":"<pre><code>cm run script --tags=run-mlperf,inference \\\n   --model=sdxl \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=datacenter \\\n   --scenario=Server \\\n   --execution-mode=valid \\\n   --device=qaic \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/text_to_image/sdxl/#run-options_34","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"benchmarks/text_to_image/sdxl/#all-scenarios_11","title":"# All Scenarios","text":"<pre><code>cm run script --tags=run-mlperf,inference,_all-scenarios \\\n   --model=sdxl \\\n   --implementation=qualcomm \\\n   --framework=glow \\\n   --category=datacenter  \\\n   --execution-mode=valid \\\n   --device=qaic \\\n   --quiet\n</code></pre>"},{"location":"benchmarks/text_to_image/sdxl/#run-options_35","title":"Run Options","text":"<ul> <li> <p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p> </li> <li> <p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p> </li> </ul>"},{"location":"changelog/","title":"What's New, What's Coming","text":""},{"location":"changelog/changelog/","title":"Release Notes","text":""},{"location":"demos/","title":"Demos","text":""},{"location":"install/","title":"Installation","text":"<p>We use MLCommons CM Automation framework to run MLPerf inference benchmarks.</p>"},{"location":"install/#cm-install","title":"CM Install","text":"<p>We have successfully tested CM on</p> <ul> <li>Ubuntu 18.x, 20.x, 22.x , 23.x, </li> <li>RedHat 8, RedHat 9, CentOS 8</li> <li>macOS</li> <li>Wndows 10, Windows 11</li> </ul> UbuntuRed HatmacOSWindows <p>Please visit the official CM installation page for more details</p>"},{"location":"install/#ubuntu-debian","title":"Ubuntu, Debian","text":"<pre><code>   sudo apt update &amp;&amp; sudo apt upgrade\n   sudo apt install python3 python3-pip python3-venv git wget curl\n</code></pre> <p>Note that you must set up virtual env on Ubuntu 23+ before using any Python project: <pre><code>   python3 -m venv cm\n   source cm/bin/activate\n</code></pre></p> <p>You can now install CM via PIP:</p> <pre><code>   python3 -m pip install cmind\n</code></pre> <p>You might need to do the following command to update the <code>PATH</code> to include the BIN paths from pip installs</p> <pre><code>   source $HOME/.profile\n</code></pre> <p>You can check that CM is available by checking the <code>cm</code> command</p>"},{"location":"install/#red-hat","title":"Red Hat","text":"<pre><code>   sudo dnf update\n   sudo dnf install python3 python-pip git wget curl\n   python3 -m pip install cmind --user\n</code></pre>"},{"location":"install/#macos","title":"macOS","text":"<p>Note that CM currently does not work with Python installed from the Apple Store.  Please install Python via brew as described below.</p> <p>If <code>brew</code> package manager is not installed, please install it as follows (see details here): <pre><code>   /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n</code></pre></p> <p>Don't forget to add brew to PATH environment as described in the end of the installation output.</p> <p>Then install python, pip, git and wget:</p> <pre><code>   brew install python3 git wget curl\n   python3 -m pip install cmind\n</code></pre>"},{"location":"install/#windows","title":"Windows","text":"<ul> <li>Configure Windows 10+ to support long paths from command line as admin:    <pre><code>   reg add \"HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Control\\FileSystem\" /v LongPathsEnabled /t REG_DWORD /d 1 /f\n</code></pre> </li> <li>Download and install Git from git-for-windows.github.io.</li> <li>Configure Git to accept long file names: <code>git config --system core.longpaths true</code></li> <li>Download and install Python 3+ from www.python.org/downloads/windows.</li> <li>Don't forget to select option to add Python binaries to PATH environment!</li> <li> <p>Configure Windows to accept long fie names during Python installation!</p> </li> <li> <p>Install CM via PIP:</p> </li> </ul> <pre><code>   python -m pip install cmind\n</code></pre> <p>Note that we have reports   that CM does not work when Python was first installed from the Microsoft Store.  If CM fails to run, you can find a fix here.</p>"},{"location":"install/#download-the-cm-mlops-repository","title":"Download the CM MLOps Repository","text":"<pre><code>   cm pull repo gateoverflow@cm4mlops\n</code></pre> <p>Now, you are ready to use the <code>cm</code> commands to run MLPerf inference as given in the benchmarks page</p>"},{"location":"submission/","title":"Submission Generation","text":"<p>If you follow the <code>cm run</code> commands under the individual model pages in the benchmarks directory, all the valid results will get aggregated to the <code>cm cache</code> folder. Once all the results across all the modelsare ready you can use the following command to generate a valid submission tree compliant with the MLPerf requirements.</p>"},{"location":"submission/#generate-actual-submission-tree","title":"Generate actual submission tree","text":"Closed EdgeClosed DatacenterOpen EdgeOpen Datacenter <ul> <li> <p>Use <code>--hw_name=\"My system name\"</code> to give a meaningful system name. Examples can be seen here</p> </li> <li> <p>Use <code>--submitter=&lt;Your name&gt;</code> if your organization is an official MLCommons member and would like to submit under your organization</p> </li> <li> <p>Use <code>--hw_notes_extra</code> option to add additional notes like <code>--hw_notes_extra=\"Result taken by NAME\"</code></p> </li> </ul> <p>The above command should generate \"submission.tar.gz\" if there are no submission checker issues and you can upload it to the MLCommons Submission UI.</p>"},{"location":"submission/#closed-edge-submission","title":"Closed Edge Submission","text":"<pre><code>   cm run script -tags=generate,inference,submission \\\n  --clean \\\n  --preprocess_submission=yes \\\n  --run-checker \\\n  --submitter=MLCommons \\\n  --tar=yes \\\n  --env.CM_TAR_OUTFILE=submission.tar.gz \\\n  --division=closed \\\n  --category=edge \\\n  --env.CM_DETERMINE_MEMORY_CONFIGURATION=yes \\\n  --quiet\n</code></pre>"},{"location":"submission/#closed-datacenter-submission","title":"Closed Datacenter Submission","text":"<pre><code>   cm run script -tags=generate,inference,submission \\\n  --clean \\\n  --preprocess_submission=yes \\\n  --run-checker \\\n  --submitter=MLCommons \\\n  --tar=yes \\\n  --env.CM_TAR_OUTFILE=submission.tar.gz \\\n  --division=closed \\\n  --category=datacenter \\\n  --env.CM_DETERMINE_MEMORY_CONFIGURATION=yes \\\n  --quiet\n</code></pre>"},{"location":"submission/#open-edge-submission","title":"Open Edge Submission","text":"<pre><code>   cm run script -tags=generate,inference,submission \\\n  --clean \\\n  --preprocess_submission=yes \\\n  --run-checker \\\n  --submitter=MLCommons \\\n  --tar=yes \\\n  --env.CM_TAR_OUTFILE=submission.tar.gz \\\n  --division=open \\\n  --category=edge \\\n  --env.CM_DETERMINE_MEMORY_CONFIGURATION=yes \\\n  --quiet\n</code></pre>"},{"location":"submission/#closed-datacenter-submission_1","title":"Closed Datacenter Submission","text":"<pre><code>   cm run script -tags=generate,inference,submission \\\n  --clean \\\n  --preprocess_submission=yes \\\n  --run-checker \\\n  --submitter=MLCommons \\\n  --tar=yes \\\n  --env.CM_TAR_OUTFILE=submission.tar.gz \\\n  --division=open \\\n  --category=datacenter \\\n  --env.CM_DETERMINE_MEMORY_CONFIGURATION=yes \\\n  --quiet\n</code></pre>"},{"location":"submission/#aggregate-results-in-github","title":"Aggregate Results in GitHub","text":"<p>If you are collecting results across multiple systems you can generate different submissions and aggregate all of them to a GitHub repository (can be private) and use it to generate a single tar ball which can be uploaded to the MLCommons Submission UI. </p> <p>Run the following command after replacing <code>--repo_url</code> with your GitHub repository URL.</p> <pre><code>   cm run script --tags=push,github,mlperf,inference,submission \\\n   --repo_url=https://github.com/GATEOverflow/mlperf_inference_submissions_v4.1 \\\n   --commit_message=\"Results on &lt;HW name&gt; added by &lt;Name&gt;\" \\\n   --quiet\n</code></pre> <p>At the end, you can download the github repo and upload to the MLCommons Submission UI.</p>"},{"location":"submission/tools-readme/","title":"Tools to check Submissions","text":""},{"location":"submission/tools-readme/#truncate_accuracy_logpy-mandatory","title":"<code>truncate_accuracy_log.py</code> (Mandatory)","text":""},{"location":"submission/tools-readme/#inputs","title":"Inputs","text":"<p>input: Path to the directory containing your submission. <code>closed</code>, <code>open</code> directories must be inside this directory.  output: Path to the directory to output the submission with truncated files  submitter: Organization name  backup: Path to the directory to store an unmodified copy of the truncated files"},{"location":"submission/tools-readme/#summary","title":"Summary","text":"<p>Takes a directory containing a submission and truncates <code>mlperf_log_accuracy.json</code> files. There are two ways to use this script. First, we could create a new submission directory with the truncated files by running: <pre><code>python truncate_accuracy_log.py --input &lt;original_submission_directory&gt; --submitter &lt;organization_name&gt; --output &lt;new_submission_directory&gt;\n</code></pre> Second, we could truncate the desired files and place and store a copy of the unmodified files in the backup repository. <pre><code>python tools/submission/truncate_accuracy_log.py --input &lt;original_submission_directory&gt; --submitter &lt;organization_name&gt; --backup &lt;safe_directory&gt; \n</code></pre></p>"},{"location":"submission/tools-readme/#outputs","title":"Outputs","text":"<p>Output directory with submission with truncated <code>mlperf_log_accuracy.json</code> files</p>"},{"location":"submission/tools-readme/#preprocess_submissionpy-optional","title":"<code>preprocess_submission.py</code> (Optional)","text":""},{"location":"submission/tools-readme/#inputs_1","title":"Inputs","text":"<p>input: Path to directory containing your submission  submitter: Organization name </p>"},{"location":"submission/tools-readme/#summary_1","title":"Summary","text":"<p>The input submission directory is modified with empty directories removed and low accuracy results inferred. Multistream and offline scenario results are also wherever possible. The original input directory is saved in a timestamped directory.</p>"},{"location":"submission/tools-readme/#submission_checkerpy-mandatory","title":"<code>submission_checker.py</code> (Mandatory)","text":""},{"location":"submission/tools-readme/#inputs_2","title":"Inputs","text":"<p>input: Path to the directory containing one or several submissions. version: Checker version. E.g v1.1, v2.0, v2.1, v3.0, v3.1.  submitter: Filter submitters and only run the checks for some specific submitter.  csv: Output path where the csv with the results will be stored. E.g <code>results/summary.csv</code>.  skip_compliance: Flag to skip compliance checks.  extra-model-benchmark-map: Extra mapping for model name to benchmarks. E.g <code>retinanet:ssd-large;efficientnet:ssd-small</code> submission-exceptions: Flag to ignore errors in submissions</p> <p>The below input fields are off by default since v3.1 and are mandatory but can be turned on for debugging purposes</p> <p>skip-power-check: Flag to skip the extra power checks. This flag has no effect on non-power submission results</p> <p>skip-meaningful-fields-emptiness-check: Flag to avoid checking if mandatory system description fields are empty</p> <p>skip-empty-files-check: Flag to avoid checking if mandatory measurement files are empty</p> <p>skip-check-power-measure-files: Flag to avoid checking if the required power measurement files are present</p>"},{"location":"submission/tools-readme/#summary_2","title":"Summary","text":"<p>Checks a directory that contains one or several submissions. This script can be used by running the following command: <pre><code>python3 submission_checker.py --input &lt;path-to-folder&gt; \n    [--version &lt;version&gt;]\n    [--submitter &lt;submitter-name&gt;]\n    [--csv &lt;path-to-output&gt;]\n    [--skip_compliance]\n    [--extra-model-benchmark-map &lt;extra-mapping-string&gt;]\n    [--submission-exceptions]\n</code></pre></p>"},{"location":"submission/tools-readme/#outputs_1","title":"Outputs","text":"<ul> <li>CSV file containing all the valid results in the directory.</li> <li>It raises several errors and logs invalid results.</li> </ul>"},{"location":"submission/tools-readme/#generate_final_reportpy-optional","title":"<code>generate_final_report.py</code> (Optional)","text":""},{"location":"submission/tools-readme/#inputs_3","title":"Inputs","text":"<p>input: Path to .csv output file of the submission checker</p>"},{"location":"submission/tools-readme/#summary_3","title":"Summary","text":"<p>Generates the spreadsheet with the format in which the final results will be published. This script can be used by running the following command: <pre><code>python3 generate_final_report.py --input &lt;path-to-csv&gt;\n</code></pre></p>"},{"location":"submission/tools-readme/#outputs_2","title":"Outputs","text":"<p>Spreadsheet with the results.</p>"},{"location":"submission/tools-readme/#log_parserpy","title":"<code>log_parser.py</code>","text":""},{"location":"submission/tools-readme/#summary_4","title":"Summary","text":"<p>Helper module for the submission checker. It parses the logs containing the results of the benchmark.</p>"},{"location":"submission/tools-readme/#filter_errorspy-deprecated","title":"<code>filter_errors.py</code> (Deprecated)","text":""},{"location":"submission/tools-readme/#summary_5","title":"Summary","text":"<p>Tool to remove manually verified ERRORs from the log file in the v0.7 submission.</p>"},{"location":"submission/tools-readme/#pack_submissionsh-deprecated","title":"<code>pack_submission.sh</code> (Deprecated)","text":""},{"location":"submission/tools-readme/#summary_6","title":"Summary","text":"<p>Creates an encrypted tarball and generates the SHA1 of the tarball. Currently submissions do not need to be encrypted.</p>"},{"location":"submission/tools-readme/#repository_checkssh-deprecated","title":"<code>repository_checks.sh</code> (Deprecated)","text":""},{"location":"submission/tools-readme/#inputs_4","title":"Inputs","text":"<p>Takes as input the path of the directory to run the checks on.</p>"},{"location":"submission/tools-readme/#summary_7","title":"Summary","text":"<p>Checks that a directory containing one or several submissions can be uploaded to github. This script can be used by running the following command: <pre><code>./repository_checks.sh &lt;path-to-folder&gt;\n</code></pre></p>"},{"location":"submission/tools-readme/#outputs_3","title":"Outputs","text":"<p>Logs in the console the errors that could cause problems uploading the submission to github.</p>"},{"location":"usage/","title":"Using CM for MLPerf Inference","text":""}]}